{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING CONCEPTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "---\n",
    "\n",
    "In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be \"ON\" ($1$) or \"OFF\" ($0$), depending on input. This is similar to the behavior of the linear perceptron in neural networks. However, only nonlinear activation functions allow such networks to compute nontrivial problems using only a small number of nodes, and such activation functions are called nonlinearities.\n",
    "\n",
    "The most common activation functions can be divided in three categories: ridge functions, radial functions and fold functions.\n",
    "\n",
    "## Ridge functions\n",
    "\n",
    "Ridge functions are univariate functions acting on a linear combination of the input variables.\n",
    "\n",
    "Often used examples include:\n",
    "\n",
    "* Linear activation: $\\large \\phi (\\mathbf{v}) = a + \\mathbf{v}'\\mathbf{b}$\n",
    "* ReLU activation: $\\large \\phi (\\mathbf{v}) = \\max(0, a + \\mathbf{v}'\\mathbf{b})$\n",
    "* Heaviside activation: $\\large \\phi (\\mathbf{v}) = 1_{a \\, + \\, \\mathbf{v}'\\mathbf{b} \\, > \\, 0}$\n",
    "* Logistic activation: $\\large \\phi (\\mathbf{v}) = (1 + \\exp(-a -\\mathbf{v}'\\mathbf{b}))^{-1}$\n",
    "\n",
    "![Activation Functions](images\\activation_functions.gif \"Activation Functions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Algorithm\n",
    "---\n",
    "\n",
    "Adam (\"Adaptive Moment Estimation\") is different to classical stochastic gradient descent. Stochastic gradient descent maintains a single learning rate (termed alpha) for all weight updates and the learning rate does not change during training. Instead, with Adam a learning rate is maintained for each network weight (parameter) and separately adapted as learning unfolds.\n",
    "\n",
    "The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. Specifically:\n",
    "\n",
    "* _Adaptive Gradient Algorithm_ (AdaGrad) that maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language and computer vision problems).\n",
    "\n",
    "* _Root Mean Square Propagation_ (RMSProp) that also maintains per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing). This means the algorithm does well on online and non-stationary problems (e.g. noisy).\n",
    "\n",
    "Adam realizes the benefits of both AdaGrad and RMSProp and is being adopted for benchmarks in deep learning papers.\n",
    "\n",
    "![Adam Comparison to Other Optimization Algorithms](images\\optimization_algorithms_comparison.png \"Adam Comparison to Other Optimization Algorithms\")\n",
    "\n",
    "\n",
    "## Configuration Parameters\n",
    "\n",
    "* __alpha__<br>\n",
    "Also referred to as the learning rate or step size. The proportion that weights are updated (e.g. $0.001$). Larger values (e.g. $0.3$) results in faster initial learning before the rate is updated. Smaller values (e.g. $1.0E-5$) slow learning right down during training\n",
    "\n",
    "* __beta1__<br>\n",
    "The exponential decay rate for the first moment estimates (e.g. $0.9$).\n",
    "\n",
    "* __beta2__<br>\n",
    "The exponential decay rate for the second-moment estimates (e.g. $0.999$). This value should be set close to $1.0$ on problems with a sparse gradient (e.g. NLP and computer vision problems).\n",
    "\n",
    "* __epsilon__<br>\n",
    "Is a very small number to prevent any division by zero in the implementation (e.g. $10E-8$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "---\n",
    "\n",
    "An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name.\n",
    "\n",
    "An autoencoder is essentially a neural network that learns to copy its input to its output. It has an internal (hidden) layer that describes a code used to represent the input, and it is constituted by two main parts: an encoder that maps the input into the code, and a decoder that maps the code to a reconstruction of the original input. Several variants exist to the basic model, with the aim of forcing the learned representations of the input to assume useful properties.\n",
    "\n",
    "![Autoencoder](images\\autoencoder_2.png \"Autoencoder\")\n",
    "\n",
    "Examples are the regularized autoencoders (Sparse, Denoising and Contractive autoencoders), proven effective in learning representations for subsequent classification tasks, and Variational autoencoders, with their recent applications as generative models. Autoencoders are effectively used for solving many applied problems, from face recognition to acquiring the semantic meaning of words.\n",
    "\n",
    "The simplest form of an autoencoder is a feedforward, non-recurrent neural network similar to single layer perceptrons that participate in multilayer perceptrons (MLP) – having an input layer, an output layer and one or more hidden layers connecting them – where the output layer has the same number of nodes (neurons) as the input layer, and with the purpose of reconstructing its inputs (minimizing the difference between the input and the output) instead of predicting the target value $Y$ given inputs $X$. Therefore, autoencoders are unsupervised learning models (do not require labeled inputs to enable learning).\n",
    "\n",
    "Once an autoencoder is trained, the encoder part of the network can be discarded and the decoder part can be used to generate new data in the observed space by creating random samples of data in latent space and mapping them to observed space. This is the core idea of generative models. There are similarities between autoencoders and Boltzmann Machine (BM). Like autoencoders, BMs are useful to extract latent space from the data. The difference is in the architecture, the representation of the latent space and the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "---\n",
    "\n",
    "Deep Learning is a particular kind of machine learning that achieves great power and flexibility by learning to represent the world as nested hierarchy of concepts, with each concept defined in relation to simpler concepts, and more abstract representations computed in terms of less abstract ones.\n",
    "\n",
    "![Deep Learning](images\\deep_learning.png \"Deep Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent (Classic) Algorithm\n",
    "---\n",
    "\n",
    "The goal of gradient descent is usually to minimize the loss function for a machine learning problem. A good algorithm finds the minimum fast and reliably well (i.e. it doesn’t get stuck in local minima, saddle points, or plateau regions, but rather goes for the global minimum).\n",
    "\n",
    "The basic gradient descent algorithm follows the idea that the opposite direction of the gradient points to where the lower area is. So it iteratively takes steps in the opposite directions of the gradients.\n",
    "\n",
    "![Gradient Descent Algorithm](images\\gradient_descent.gif \"Gradient Descent Algorithm\")\n",
    "\n",
    "As human perception is limited to 3 dimensions, in all our visualizations, imagine we only have two parameters (or thetas) to optimize, and they are represented by the $x$ and $y$ dimensions in the graph. The surface is the loss function. We want to find the ($x$, $y$) combination that’s at the lowest point of the surface. The problem is trivial to us because we can see the whole surface. But the ball (the descent algorithm) doesn’t; it can only take one step at a time and explore its surroundings, analogous to walking in the dark with only a flashlight.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space\n",
    "---\n",
    "\n",
    "The concept of “latent space” is important because it’s utility is at the core of Deep Learning - learning the features of data and simplifying data representations for the purpose of finding patterns. If it seems that this process is \"hidden\" it’s because it is. Latent, by definition, means \"hidden\".\n",
    "\n",
    "For example, let's say we train a model to classify an image using a fully convolutional neural network. This could be to output a digit number when given an image of a digit. As the model \"learns\", it is simply learning features at each layer (edges, angles, etc.) and attributing a combination of features to a specific output. Each time the model learns through a data point, the dimensionality of the image is first reduced before it is ultimately increased (through an Encoder and a Bottleneck).\n",
    "\n",
    "![Latent Space Representation](images\\latent_space.png \"Latent Space Representaion\")\n",
    "\n",
    "When the dimensionality is reduced, it is considered a form of lossy compression. That said, data is often compressed in machine learning to learn important information about data points. This compressed state is the \"Latent Space Representation\" of the data. Because the model is required to then reconstruct the compressed data (through a Decoder), it must learn to store all relevant information and disregard the noise. This is the value of compression - it allows us to get rid of any extraneous information, and only focus on the most important features.\n",
    "\n",
    "In other words, the model learns the data features and simplifies its representation to make it easier to analyze.\n",
    "This is at the core of a concept called Representation Learning, defined as a set of techniques that allow a system to discover the representations needed for feature detection or classification from raw data.\n",
    "\n",
    "The latent space is an essential concept in manifold learning, a subfield of representation learning. Manifolds in data science can be understood as groups or subsets of data that are \"similar\" in some way. These similarities, usually imperceptible or obscured in higher-dimensional space, can be discovered once data has been represented in the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chain\n",
    "---\n",
    "\n",
    "A Markov chain is a probabilistic model used to estimate a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. In a Markov chain, the future state depends only on the present state and not on the past states.\n",
    "\n",
    "An example of Markov’s process is the position of a randomly walking person when at instant $t+1$ would be dependent on the current state $t$ and not on the previous states ($t-1$, $t-2$, $\\cdots$). This behavior is referred to as Markov property.\n",
    "\n",
    "![Markov Chain](images\\markov_chain.png \"Markov Chain\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum Algorithm\n",
    "---\n",
    "\n",
    "The gradient descent with momentum algorithm (or Momentum for short) borrows the idea from physics. Imagine rolling down a ball inside of a frictionless bowl. Instead of stopping at the bottom, the momentum it has accumulated pushes it forward, and the ball keeps rolling back and forth.\n",
    "\n",
    "![Momentum Algorithm](images\\momentum.gif \"Momentum Algorithm\")\n",
    "\n",
    "We can apply the concept of momentum to our vanilla gradient descent algorithm. In each step, in addition to the regular gradient, it also adds on the movement from the previous step. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nominal vs Ordinal Data\n",
    "---\n",
    "\n",
    "Nominal data are those items which are distinguished by a simple naming system. They are data with no numeric value, such as profession. The nominal data just name a thing without applying it to an order related to other numbered items.\n",
    "\n",
    "The most popular way of thinking about nominal data and variables is that they are just named.\n",
    "\n",
    "Nominal data are also called categorical data. In the nominal scale, the subjects are only allocated to different categories. The values grouped into these categories have no meaningful order. There is no hierarchy. For example, gender and occupation are nominal level values.\n",
    "\n",
    "Ordinal data is data which is placed into some kind of order by their position on the scale. For example, they may indicate superiority. However, you cannot do arithmetic with ordinal numbers because they only show sequence.\n",
    "\n",
    "Ordinal data and variables are considered as “in between” categorical and quantitative variables. In other words, the ordinal data is categorical data for which the values are ordered.\n",
    "\n",
    "![Nominal vs Ordinal Data](images\\nominal_vs_ordinal_data.png \"Nominal vs Ordinal Data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid\n",
    "---\n",
    "\n",
    "A sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve.\n",
    "\n",
    "A wide variety of sigmoid functions including the logistic and hyperbolic tangent functions have been used as the activation function of artificial neurons. Sigmoid curves are also common in statistics as cumulative distribution functions (which go from $0$ to $1$), such as the integrals of the logistic density, the normal density, and Student's $t$ probability density functions. The logistic sigmoid function is invertible, and its inverse is the logit function.\n",
    "\n",
    "In general, a sigmoid function is monotonic, and has a first derivative which is bell shaped. Conversely, the integral of any continuous, non-negative, bell-shaped function (with one local maximum and no local minimum, unless degenerate) will be sigmoidal. Thus the cumulative distribution functions for many common probability distributions are sigmoidal. One such example is the error function, which is related to the cumulative distribution function of a normal distribution.\n",
    "\n",
    "A sigmoid function is constrained by a pair of horizontal asymptotes as $x \\rightarrow \\pm \\infty$.\n",
    "\n",
    "A sigmoid function is convex for values less than $0$, and it is concave for values greater than $0$.\n",
    "\n",
    "![Sigmoid Functions](images\\sigmoid_functions.png \"Sigmoid Functions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NOTB",
   "language": "python",
   "name": "notb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
