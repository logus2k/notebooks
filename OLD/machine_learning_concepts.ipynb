{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING CONCEPTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "---\n",
    "\n",
    "In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be \"ON\" ($1$) or \"OFF\" ($0$), depending on input. This is similar to the behavior of the linear perceptron in neural networks. However, only nonlinear activation functions allow such networks to compute nontrivial problems using only a small number of nodes, and such activation functions are called nonlinearities.\n",
    "\n",
    "The most common activation functions can be divided in three categories: ridge functions, radial functions and fold functions.\n",
    "\n",
    "## Ridge functions\n",
    "\n",
    "Ridge functions are univariate functions acting on a linear combination of the input variables.\n",
    "\n",
    "Often used examples include:\n",
    "\n",
    "* Linear activation: $\\large \\phi (\\mathbf{v}) = a + \\mathbf{v}'\\mathbf{b}$\n",
    "* ReLU activation: $\\large \\phi (\\mathbf{v}) = \\max(0, a + \\mathbf{v}'\\mathbf{b})$\n",
    "* Heaviside activation: $\\large \\phi (\\mathbf{v}) = 1_{a \\, + \\, \\mathbf{v}'\\mathbf{b} \\, > \\, 0}$\n",
    "* Logistic activation: $\\large \\phi (\\mathbf{v}) = (1 + \\exp(-a -\\mathbf{v}'\\mathbf{b}))^{-1}$\n",
    "\n",
    "![Activation Functions](images\\activation_functions.gif \"Activation Functions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Algorithm\n",
    "---\n",
    "\n",
    "Adam (\"Adaptive Moment Estimation\") is different to classical stochastic gradient descent. Stochastic gradient descent maintains a single learning rate (termed alpha) for all weight updates and the learning rate does not change during training. Instead, with Adam a learning rate is maintained for each network weight (parameter) and separately adapted as learning unfolds.\n",
    "\n",
    "The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. Specifically:\n",
    "\n",
    "* _Adaptive Gradient Algorithm_ (AdaGrad) that maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language and computer vision problems).\n",
    "\n",
    "* _Root Mean Square Propagation_ (RMSProp) that also maintains per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing). This means the algorithm does well on online and non-stationary problems (e.g. noisy).\n",
    "\n",
    "Adam realizes the benefits of both AdaGrad and RMSProp and is being adopted for benchmarks in deep learning papers.\n",
    "\n",
    "![Adam Comparison to Other Optimization Algorithms](images\\optimization_algorithms_comparison.png \"Adam Comparison to Other Optimization Algorithms\")\n",
    "\n",
    "\n",
    "## Configuration Parameters\n",
    "\n",
    "* __alpha__<br>\n",
    "Also referred to as the learning rate or step size. The proportion that weights are updated (e.g. $0.001$). Larger values (e.g. $0.3$) results in faster initial learning before the rate is updated. Smaller values (e.g. $1.0E-5$) slow learning right down during training\n",
    "\n",
    "* __beta1__<br>\n",
    "The exponential decay rate for the first moment estimates (e.g. $0.9$).\n",
    "\n",
    "* __beta2__<br>\n",
    "The exponential decay rate for the second-moment estimates (e.g. $0.999$). This value should be set close to $1.0$ on problems with a sparse gradient (e.g. NLP and computer vision problems).\n",
    "\n",
    "* __epsilon__<br>\n",
    "Is a very small number to prevent any division by zero in the implementation (e.g. $10E-8$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "---\n",
    "\n",
    "An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name.\n",
    "\n",
    "An autoencoder is essentially a neural network that learns to copy its input to its output. It has an internal (hidden) layer that describes a code used to represent the input, and it is constituted by two main parts: an encoder that maps the input into the code, and a decoder that maps the code to a reconstruction of the original input. Several variants exist to the basic model, with the aim of forcing the learned representations of the input to assume useful properties.\n",
    "\n",
    "![Autoencoder](images\\autoencoder_2.png \"Autoencoder\")\n",
    "\n",
    "Examples are the regularized autoencoders (Sparse, Denoising and Contractive autoencoders), proven effective in learning representations for subsequent classification tasks, and Variational autoencoders, with their recent applications as generative models. Autoencoders are effectively used for solving many applied problems, from face recognition to acquiring the semantic meaning of words.\n",
    "\n",
    "The simplest form of an autoencoder is a feedforward, non-recurrent neural network similar to single layer perceptrons that participate in multilayer perceptrons (MLP) – having an input layer, an output layer and one or more hidden layers connecting them – where the output layer has the same number of nodes (neurons) as the input layer, and with the purpose of reconstructing its inputs (minimizing the difference between the input and the output) instead of predicting the target value $Y$ given inputs $X$. Therefore, autoencoders are unsupervised learning models (do not require labeled inputs to enable learning).\n",
    "\n",
    "Once an autoencoder is trained, the encoder part of the network can be discarded and the decoder part can be used to generate new data in the observed space by creating random samples of data in latent space and mapping them to observed space. This is the core idea of generative models. There are similarities between autoencoders and Boltzmann Machine (BM). Like autoencoders, BMs are useful to extract latent space from the data. The difference is in the architecture, the representation of the latent space and the training process."
   ]
  },
  {
   "source": [
    "# Bias\n",
    "---\n",
    "\n",
    "![Network Bias](images\\network_bias.png \"Network Bias\")"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Cost Function\n",
    "---\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "$\\Large J(\\theta_{0}, \\theta_{1}) = \\frac{1}{2m} \\sum\\limits_{i=1}^{m} (\\hat{y}_{i} - y_{i})^{2} = \\frac{1}{2m} \\sum\\limits_{i=1}^{m} (h_{\\theta}(x_{i}) - y_{i})^{2}$\n",
    "\n",
    "We can measure the accuracy of our linear regression hypothesis function ($h_{\\theta}$) by using a cost function ($J$). This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from $x$'s and the actual output $y$'s.\n",
    "\n",
    "To break it apart, it is $\\frac{1}{2}\\bar{x}$ where $\\bar{x}$ is the mean of the squares of $h_\\theta (x_{i}) - y_{i}$, or the difference between the predicted value and the actual value.\n",
    "\n",
    "This function is otherwise called the \"Squared Error Function\", or \"Mean Squared Error\". The mean is halved $\\left(\\frac{1}{2}\\right)$ as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the $\\frac{1}{2}$ term.\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "$\\Large J(\\theta) = \\frac{1}{m} \\sum\\limits_{i=1}^{m} Cost(h_{\\theta}(x_{i}), y_{i})$\n",
    "\n",
    "In logistic regression, the cost function can be expressed as:\n",
    "\n",
    "$\\large Cost(h_{\\theta}(x), y) = -\\log(h_{\\theta}(x)) \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, if \\, \\, \\, y = 1$\n",
    "\n",
    "$\\large Cost(h_{\\theta}(x), y) = -\\log(1 - h_{\\theta}(x)) \\, \\, \\, \\, \\, \\, \\ \\, \\, \\, \\, \\, if \\, \\, \\, y = 0$\n",
    "\n",
    "Or, using a more compressed version that fits in a single line:\n",
    "\n",
    "$\\large Cost(h_{\\theta}(x), y) = -y \\log(h_{\\theta}(x)) - ((1 - y) \\log(1 - h_{\\theta}(x))$\n",
    "\n",
    "So that the full logistic regression formula (including the compact version of its cost function) becomes:\n",
    "\n",
    "$\\Large J(\\theta) = \\frac{1}{m} [ \\, \\sum\\limits_{i=1}^{m} y_{i} \\log h_{\\theta}(x_{i}) + (1 - y_{i}) \\log(1 - h_{\\theta}(x_{i})) \\, ]$\n",
    "\n",
    "Noting that we cannot use the same cost function that we use for linear regression because the Logistic Function would cause the output to be wavy, causing many local optima. In other words, it would not be a convex function.\n",
    "\n",
    "If our correct answer $y$ is $0$, then the cost function will be $0$ if our hypothesis function also outputs $0$. If our hypothesis approaches $1$, then the cost function will approach infinity.\n",
    "\n",
    "If our correct answer $y$ is $1$, then the cost function will be $0$ if our hypothesis function outputs $1$. If our hypothesis approaches $0$, then the cost function will approach infinity.\n",
    "\n",
    "Note that writing the cost function in this way guarantees that $J(\\theta)$ is convex for logistic regression.\n",
    "\n",
    "## Examples\n",
    "\n",
    "TBD"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "---\n",
    "\n",
    "Deep Learning is a particular kind of machine learning that achieves great power and flexibility by learning to represent the world as nested hierarchy of concepts, with each concept defined in relation to simpler concepts, and more abstract representations computed in terms of less abstract ones.\n",
    "\n",
    "![Deep Learning](images\\deep_learning.png \"Deep Learning\")"
   ]
  },
  {
   "source": [
    "# Epoch\n",
    "---\n",
    "\n",
    "Number epoch equal to the number of times the algorithm sees the entire data set. So, each time the algorithm has seen all samples in the dataset, one epoch has completed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "---\n",
    "\n",
    "## Formula\n",
    "\n",
    "The general form of gradient descent is:\n",
    "\n",
    "$\\LARGE \\theta_{j} := \\theta_{j} - \\alpha \\frac{\\partial}{\\partial\\theta_{j}} J(\\theta_{0}, \\theta_{1})$\n",
    "\n",
    "Using Calculus, we can work out the derivative part to get:\n",
    "\n",
    "$\\large Repeat \\, \\, \\{ \\\\\n",
    "    \\LARGE \\, \\, \\, \\, \\, \\, \\, \\theta_{j} := \\theta_{j} - \\frac{\\alpha}{m} \\sum\\limits_{i=1}^{m} (h_{\\theta}(x_{i}) - y_{i}) x_{j}\\\\\n",
    "\\large \\}$\n",
    "\n",
    "The goal of gradient descent is usually to minimize the loss function for a machine learning problem. A good algorithm finds the minimum fast and reliably well (i.e. it doesn’t get stuck in local minima, saddle points, or plateau regions, but rather goes for the global minimum). \n",
    "\n",
    "In the formula above, $\\alpha$ is the learning rate (it will be used as the step size of the gradient descendant). If $\\alpha$ is too small, the gradient descent will be slow. If $\\alpha$ is too large, the gradient descent can overshoot (miss or pass over) the minimum (it may fail to converge, or even diverge). It is also important to note that the gradient descent can converge to a local minimum even having the learning rate $\\alpha$ as a fixed value, because as it approaches the local minimum, it will automatically take smaller steps, so there is no need to decrease $\\alpha$ over time.\n",
    "\n",
    "The $\\frac{\\partial}{\\partial\\theta_{j}} J(\\theta_{0}, \\theta_{1})$ part of the formula is just the derivative (the slope). Regardless of the slope's sign, the derivative eventually converges to its minimum value, therefore, by definition, will return $0$ when the gradient descent reaches the local minimum. When the slope is negative, the value of $\\theta_{1}$ increases and when it is positive, the value of $\\theta_{1}$ decreases.\n",
    "\n",
    "![Gradient Descent Algorithm](images\\gradient_descent.gif \"Gradient Descent Algorithm\")\n",
    "\n",
    "The basic gradient descent algorithm follows the idea that the opposite direction of the gradient points to where the lower area is. So it iteratively takes steps in the opposite directions of the gradients.\n",
    "\n",
    "As human perception is limited to 3 dimensions, in all our visualizations, imagine we only have two parameters (or thetas) to optimize, and they are represented by the $x$ and $y$ dimensions in the graph. The surface is the loss function. We want to find the ($x$, $y$) combination that’s at the lowest point of the surface. The problem is trivial to us because we can see the whole surface. But the ball (the descent algorithm) doesn’t; it can only take one step at a time and explore its surroundings, analogous to walking in the dark with only a flashlight.\n",
    "\n",
    "![Gradient Descent Algorithm Comparison](images\\gradient_descent_2.png \"Gradient Descent Algorithm Comparison\")\n",
    "\n",
    "A batch gradient descent means that at each step of gradient descent all the training samples will be used.\n",
    "\n",
    "![Gradient Descent Algorithm Comparison](images\\gradient_descent_1.jpeg \"Gradient Descent Algorithm Comparison\")\n",
    "\n",
    "## Feature Scaling\n",
    "\n",
    "We can speed up gradient descent by having each of our input values in roughly the same range. This is because $\\theta$ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\n",
    "\n",
    "The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally:\n",
    "\n",
    "$-1 \\leq x_{(i)} \\leq 0.5$\n",
    "\n",
    "These aren't exact requirements; we are only trying to speed things up. The goal is to get all input variables into roughly one of these ranges, give or take a few.\n",
    "\n",
    "Two techniques to help with this are feature scaling and mean normalization. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula:\n",
    "\n",
    "$\\large x_{i} := \\dfrac{x_{i} - \\mu_{i}}{s_{i}}$\n",
    "\n",
    "Where $\\mu_{i}$ is the average of all the values for feature ($i$) and $s_{i}$ is the range of values (max - min), or $s_{i}$ is the standard deviation.\n",
    "\n",
    "Note that dividing by the range, or dividing by the standard deviation, give different results. For example, if $x_{i}$ represents housing prices with a range of $100$ to $2000$ and a mean value of $1000$, then, $x_{i} := \\dfrac{price-1000}{1900}$.\n",
    "\n",
    "## Learning Rate\n",
    "\n",
    " * Debugging gradient descent: make a plot with number of iterations on the $x$-axis. Now plot the cost function, $J(\\theta)$ over the number of iterations of gradient descent. If $J(\\theta)$ ever increases, then you probably need to decrease the learning rate ($\\alpha$).\n",
    "\n",
    " * Automatic convergence test: declare convergence if $J(\\theta)$ decreases by less than $E$ in one iteration, where $E$ is some small value such as $10^{−3}$. However, in practice it's difficult to choose this threshold value.\n",
    "\n",
    "It has been proven that if learning rate $\\alpha$ is sufficiently small, then $J(\\theta)$ will decrease on every iteration. To summarize:\n",
    "\n",
    " * If $\\alpha$ is too small: slow convergence. \n",
    " * If $\\alpha$ is too large: $J(\\theta)$ may not decrease on every iteration and thus may not converge."
   ]
  },
  {
   "source": [
    "# Iteration\n",
    "---\n",
    "\n",
    "Every time a batch of data passes through the neural network, completes one iteration. In the case of neural networks, that means the forward pass and backward pass. So, epoch = batch size * number of iterations. So, one epoch includes all the training examples whereas one iteration includes only one batch of training examples.\n",
    "\n",
    "One training step is equal to process one batch of data, while all batches need to be processed to make one epoch. Steps parameter indicates the number of steps to run over data. A training step corresponds to one gradient update."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space\n",
    "---\n",
    "\n",
    "The concept of “latent space” is important because it’s utility is at the core of Deep Learning - learning the features of data and simplifying data representations for the purpose of finding patterns. If it seems that this process is \"hidden\" it’s because it is. Latent, by definition, means \"hidden\".\n",
    "\n",
    "For example, let's say we train a model to classify an image using a fully convolutional neural network. This could be to output a digit number when given an image of a digit. As the model \"learns\", it is simply learning features at each layer (edges, angles, etc.) and attributing a combination of features to a specific output. Each time the model learns through a data point, the dimensionality of the image is first reduced before it is ultimately increased (through an Encoder and a Bottleneck).\n",
    "\n",
    "![Latent Space Representation](images\\latent_space.png \"Latent Space Representaion\")\n",
    "\n",
    "When the dimensionality is reduced, it is considered a form of lossy compression. That said, data is often compressed in machine learning to learn important information about data points. This compressed state is the \"Latent Space Representation\" of the data. Because the model is required to then reconstruct the compressed data (through a Decoder), it must learn to store all relevant information and disregard the noise. This is the value of compression - it allows us to get rid of any extraneous information, and only focus on the most important features.\n",
    "\n",
    "In other words, the model learns the data features and simplifies its representation to make it easier to analyze.\n",
    "This is at the core of a concept called Representation Learning, defined as a set of techniques that allow a system to discover the representations needed for feature detection or classification from raw data.\n",
    "\n",
    "The latent space is an essential concept in manifold learning, a subfield of representation learning. Manifolds in data science can be understood as groups or subsets of data that are \"similar\" in some way. These similarities, usually imperceptible or obscured in higher-dimensional space, can be discovered once data has been represented in the latent space."
   ]
  },
  {
   "source": [
    "# Linear Regression\n",
    "---\n",
    "\n",
    "## Formula\n",
    "\n",
    "$\\LARGE h_{\\theta}(x) = \\sum\\limits_{j = 0}^{n} \\theta_{j} x_{j}$\n",
    "\n",
    "TBD\n",
    "\n",
    "## Examples\n",
    "\n",
    "TBD"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chain\n",
    "---\n",
    "\n",
    "A Markov Chain is a probabilistic model used to estimate a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. In a Markov chain, the future state depends only on the present state and not on the past states.\n",
    "\n",
    "An example of Markov’s process is the position of a randomly walking person when at instant $t+1$ would be dependent on the current state $t$ and not on the previous states ($t-1$, $t-2$, $\\cdots$). This behavior is referred to as Markov's property.\n",
    "\n",
    "![Markov Chain](images\\markov_chain.png \"Markov Chain\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum Algorithm\n",
    "---\n",
    "\n",
    "The gradient descent with momentum algorithm (or Momentum for short) borrows the idea from physics. Imagine rolling down a ball inside of a frictionless bowl. Instead of stopping at the bottom, the momentum it has accumulated pushes it forward, and the ball keeps rolling back and forth.\n",
    "\n",
    "![Momentum Algorithm](images\\momentum.gif \"Momentum Algorithm\")\n",
    "\n",
    "We can apply the concept of momentum to our vanilla gradient descent algorithm. In each step, in addition to the regular gradient, it also adds on the movement from the previous step. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nominal (Categorical) vs Ordinal Data\n",
    "---\n",
    "\n",
    "Nominal data are those items which are distinguished by a simple naming system. They are data with no numeric value, such as profession. The nominal data just name a thing without applying it to an order related to other numbered items.\n",
    "\n",
    "The most popular way of thinking about nominal data and variables is that they are just named.\n",
    "\n",
    "Nominal data are also called categorical data. In the nominal scale, the subjects are only allocated to different categories. The values grouped into these categories have no meaningful order. There is no hierarchy. For example, gender and occupation are nominal level values.\n",
    "\n",
    "Ordinal data is data which is placed into some kind of order by their position on the scale. For example, they may indicate superiority. However, you cannot do arithmetic with ordinal numbers because they only show sequence.\n",
    "\n",
    "Ordinal data and variables are considered as “in between” categorical and quantitative variables. In other words, the ordinal data is categorical data for which the values are ordered.\n",
    "\n",
    "![Nominal vs Ordinal Data](images\\nominal_vs_ordinal_data.png \"Nominal vs Ordinal Data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid\n",
    "---\n",
    "\n",
    "A sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve.\n",
    "\n",
    "A wide variety of sigmoid functions including the logistic and hyperbolic tangent functions have been used as the activation function of artificial neurons. Sigmoid curves are also common in statistics as cumulative distribution functions (which go from $0$ to $1$), such as the integrals of the logistic density, the normal density, and Student's $t$ probability density functions. The logistic sigmoid function is invertible, and its inverse is the logit function.\n",
    "\n",
    "In general, a sigmoid function is monotonic, and has a first derivative which is bell shaped. Conversely, the integral of any continuous, non-negative, bell-shaped function (with one local maximum and no local minimum, unless degenerate) will be sigmoidal. Thus the cumulative distribution functions for many common probability distributions are sigmoidal. One such example is the error function, which is related to the cumulative distribution function of a normal distribution.\n",
    "\n",
    "A sigmoid function is constrained by a pair of horizontal asymptotes as $x \\rightarrow \\pm \\infty$.\n",
    "\n",
    "A sigmoid function is convex for values less than $0$, and it is concave for values greater than $0$.\n",
    "\n",
    "![Sigmoid Functions](images\\sigmoid_functions.png \"Sigmoid Functions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NOTB",
   "language": "python",
   "name": "notb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}