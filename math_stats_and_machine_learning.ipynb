{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH, STATISTICS AND MACHINE LEARNING\n",
    "---\n",
    "# Concepts, Notation and Formulae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "---\n",
    "\n",
    "In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be \"ON\" ($1$) or \"OFF\" ($0$), depending on input. This is similar to the behavior of the linear perceptron in neural networks. However, only nonlinear activation functions allow such networks to compute nontrivial problems using only a small number of nodes, and such activation functions are called nonlinearities.\n",
    "\n",
    "So, an activation function is basically just a simple function that transforms its inputs into outputs that have a certain range. There are various types of activation functions that perform this task in a different manner, For example, the sigmoid activation function takes input and maps the resulting values in between 0 to 1.\n",
    "\n",
    "One of the reasons that this function is added into an artificial neural network in order to help the network learn complex patterns in the data. These functions introduce nonlinear real-world properties to artificial neural networks. Basically, in a simple neural network, x is defined as inputs, w weights, and we pass f (x) that is the value passed to the output of the network. This will then be the final output or the input of another layer.\n",
    "\n",
    "If the activation function is not applied, the output signal becomes a simple linear function. A neural network without activation function will act as a linear regression with limited learning power. But we also want our neural network to learn non-linear states as we give it complex real-world information such as image, video, text, and sound.\n",
    "\n",
    "The most common activation functions can be divided in three categories: ridge functions, radial functions and fold functions.\n",
    "\n",
    "## Ridge functions\n",
    "\n",
    "Ridge functions are univariate functions acting on a linear combination of the input variables.\n",
    "\n",
    "Often used examples include:\n",
    "\n",
    "* Linear activation: $\\large \\phi (\\mathbf{v}) = a + \\mathbf{v}'\\mathbf{b}$\n",
    "* ReLU activation: $\\large \\phi (\\mathbf{v}) = \\max(0, a + \\mathbf{v}'\\mathbf{b})$\n",
    "* Heaviside activation: $\\large \\phi (\\mathbf{v}) = 1_{a \\, + \\, \\mathbf{v}'\\mathbf{b} \\, > \\, 0}$\n",
    "* Logistic activation: $\\large \\phi (\\mathbf{v}) = (1 + \\exp(-a -\\mathbf{v}'\\mathbf{b}))^{-1}$\n",
    "\n",
    "![Activation Functions](images\\activation_functions.gif \"Activation Functions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Algorithm\n",
    "---\n",
    "\n",
    "Adam (\"Adaptive Moment Estimation\") is different to classical stochastic gradient descent. Stochastic gradient descent maintains a single learning rate (termed alpha) for all weight updates and the learning rate does not change during training. Instead, with Adam a learning rate is maintained for each network weight (parameter) and separately adapted as learning unfolds.\n",
    "\n",
    "The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. Specifically:\n",
    "\n",
    "* _Adaptive Gradient Algorithm_ (AdaGrad) that maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language and computer vision problems).\n",
    "\n",
    "* _Root Mean Square Propagation_ (RMSProp) that also maintains per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing). This means the algorithm does well on online and non-stationary problems (e.g. noisy).\n",
    "\n",
    "Adam realizes the benefits of both AdaGrad and RMSProp and is being adopted for benchmarks in deep learning papers.\n",
    "\n",
    "![Adam Comparison to Other Optimization Algorithms](images\\optimization_algorithms_comparison.png \"Adam Comparison to Other Optimization Algorithms\")\n",
    "\n",
    "\n",
    "## Configuration Parameters\n",
    "\n",
    "* __alpha__<br>\n",
    "Also referred to as the learning rate or step size. The proportion that weights are updated (e.g. $0.001$). Larger values (e.g. $0.3$) results in faster initial learning before the rate is updated. Smaller values (e.g. $1.0E-5$) slow learning right down during training\n",
    "\n",
    "* __beta1__<br>\n",
    "The exponential decay rate for the first moment estimates (e.g. $0.9$).\n",
    "\n",
    "* __beta2__<br>\n",
    "The exponential decay rate for the second-moment estimates (e.g. $0.999$). This value should be set close to $1.0$ on problems with a sparse gradient (e.g. NLP and computer vision problems).\n",
    "\n",
    "* __epsilon__<br>\n",
    "Is a very small number to prevent any division by zero in the implementation (e.g. $10E-8$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "---\n",
    "\n",
    "An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name.\n",
    "\n",
    "An autoencoder is essentially a neural network that learns to copy its input to its output. It has an internal (hidden) layer that describes a code used to represent the input, and it is constituted by two main parts: an encoder that maps the input into the code, and a decoder that maps the code to a reconstruction of the original input. Several variants exist to the basic model, with the aim of forcing the learned representations of the input to assume useful properties.\n",
    "\n",
    "![Autoencoder](images\\autoencoder_2.png \"Autoencoder\")\n",
    "\n",
    "Examples are the regularized autoencoders (Sparse, Denoising and Contractive autoencoders), proven effective in learning representations for subsequent classification tasks, and Variational autoencoders, with their recent applications as generative models. Autoencoders are effectively used for solving many applied problems, from face recognition to acquiring the semantic meaning of words.\n",
    "\n",
    "The simplest form of an autoencoder is a feedforward, non-recurrent neural network similar to single layer perceptrons that participate in multilayer perceptrons (MLP) – having an input layer, an output layer and one or more hidden layers connecting them – where the output layer has the same number of nodes (neurons) as the input layer, and with the purpose of reconstructing its inputs (minimizing the difference between the input and the output) instead of predicting the target value $Y$ given inputs $X$. Therefore, autoencoders are unsupervised learning models (do not require labeled inputs to enable learning).\n",
    "\n",
    "Once an autoencoder is trained, the encoder part of the network can be discarded and the decoder part can be used to generate new data in the observed space by creating random samples of data in latent space and mapping them to observed space. This is the core idea of generative models. There are similarities between autoencoders and Boltzmann Machine (BM). Like autoencoders, BMs are useful to extract latent space from the data. The difference is in the architecture, the representation of the latent space and the training process."
   ]
  },
  {
   "source": [
    "# Backpropagation\n",
    "---\n",
    "\n",
    "## Formula\n",
    "\n",
    "$\\LARGE \\frac{\\partial}{\\partial\\theta_{i, j}^{(l)}} J(\\theta)$\n",
    "\n",
    "\"Backpropagation\" is a neural-network term that refers to the process of minimizing a cost function. The goal is to compute $\\min_{\\theta} J(\\Theta)$, that is, minimize a cost function $J$ using an optimal set of parameters in $\\theta$. The formula above is used to compute the partial derivative of $J(\\theta)$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Theorem\n",
    "---\n",
    "\n",
    "## Formula\n",
    "\n",
    "$\\Large P(A \\mid B) = \\dfrac{P(B \\mid A) \\, \\cdot \\, P(A)}{P(B)}$\n",
    "\n",
    "In probability theory and statistics, Bayes's theorem, named after Reverend Thomas Bayes (also known as \"Bayes's law\" or \"Bayes's rule\"), describes the probability of an event, based on prior knowledge of conditions that might be related to that event.\n",
    "\n",
    "Bayes's theorem is stated mathematically in a equation, where $A$ and $B$ are events and $P(B) \\neq 0$:\n",
    "\n",
    "* $P(A \\mid B)$ is a conditional probability: the likelihood of event $A$ occurring given that $B$ is true.\n",
    "* $P(B \\mid A)$ is also a conditional probability: the likelihood of event $B$ occurring given that $A$ is true.\n",
    "* $P(A)$ and $P(B)$ are the probabilities of observing $A$ and $B$ respectively. They are known as the marginal probability.\n",
    "\n",
    "One of the many applications of Bayes's theorem is Bayesian inference, a particular approach to statistical inference. When applied, the probabilities involved in Bayes's theorem may have different probability interpretations. With Bayesian probability interpretation, the theorem expresses how a degree of belief, expressed as a probability, should rationally change to account for the availability of related evidence. Bayesian inference is fundamental to Bayesian statistics.\n",
    "\n",
    "## Examples\n",
    "\n",
    "If the risk of developing health problems is known to increase with age, Bayes's theorem allows the risk to an individual of a known age to be assessed more accurately (by conditioning it on his age) than simply assuming that the individual is typical of the population as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference\n",
    "---\n",
    "\n",
    "## Formula\n",
    "\n",
    "$\\Large P(H \\mid E) = \\dfrac{P(E \\mid H) \\, \\cdot \\, P(H)}{P(E)}$\n",
    "\n",
    "Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics. Bayesian updating is particularly important in the dynamic analysis of a sequence of data.\n",
    "\n",
    "Bayesian inference has found application in a wide range of activities, including science, engineering, philosophy, medicine, sport, and law. In the philosophy of decision theory, Bayesian inference is closely related to subjective probability, often called \"Bayesian probability\".\n",
    "\n",
    "## Examples\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias vs Variance\n",
    "---\n",
    "\n",
    "## Bias\n",
    "\n",
    "Bias is the difference between the average prediction of a model and the correct value which we are trying to predict. It is the accuracy of our predictions. A model with high bias consistently learns the wrong thing by paying very little attention to the training data and oversimplifies its hypothesis, thus causing underfitting. An high bias always leads to an high error rate on both training and test data. Examples of algorithms with potential high bias include: Linear Regression, Linear Discriminant Analysis, and Logistic Regression.\n",
    "\n",
    "## Variance\n",
    "\n",
    "Variance is the variability of model prediction for a given random variable or a measure which tells us the dispersion of our data. A model with high variance pays a lot of attention to training data and does not generalize well on the data which it hasn’t seen before. As a result, such model performs very well on training data but has high error rates on test data, thus causing the model's overfitting.\n",
    "\n",
    "## Debugging Learning Algorithms\n",
    "\n",
    "![Network Bias](images\\network_bias.png \"Network Bias\")\n",
    "\n",
    "Training an algorithm on a very few number of data points (such as $1$, $2$ or $3$) will easily have $0$ errors because we can always find a quadratic curve that touches exactly those number of points. Hence:\n",
    "\n",
    " * As the training set gets larger, the error for a quadratic function increases.\n",
    " * The error value will plateau out after a certain $m$, or training set size.\n",
    "\n",
    "### Underfitting\n",
    "\n",
    "In supervised learning, underfitting happens when a model is unable to capture the underlying pattern of the data. These models usually have high bias and low variance. It happens when we have very low amount of data to build an accurate model or when we try to build a linear model using nonlinear data. Very simple models are prone to underfitting.\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "In supervised learning, overfitting happens when a model captures noise along with the underlying patterns in data. It happens when we train our model a lot over a noisy dataset. These models have a low bias and an high variance. Very complex models are prone to overfitting.\n",
    "\n",
    "### Regularization\n",
    "\n",
    "The regression method used to tackle high variance is called regularization. What Regularization does to overfit models is that, it negates or minimizes the effect of predictor columns with large outliers, by penalizing their regression coefficients. The result is a smoother model which can work well on other test data sets with similar kind of data.\n",
    "\n",
    "![Underfitting vs Overfitting](images\\underfitting_overfitting.png \"Underfitting vs Overfitting\")\n",
    "\n",
    "A neural network with fewer parameters is prone to underfitting. It is also computationally cheaper. A large neural network with more parameters is prone to overfitting. It is also computationally expensive. In this case, we can use regularization (increase $\\lambda$) to address the overfitting.\n",
    "\n",
    "Using a single hidden layer is a good starting default. We can train the neural network on a number of hidden layers using a cross validation set. We can then select the one that performs best. \n",
    "\n",
    "### Model Complexity Effects\n",
    "\n",
    " * Lower-order polynomials (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently.\n",
    " * Higher-order polynomials (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance.\n",
    " * In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well.\n",
    "\n",
    "### Experiencing High Bias\n",
    "\n",
    "A low training set size: causes $J_{train}(\\Theta)$ to be low and $J_{CV}(\\Theta)$ to be high. A large training set size: causes both $J_{train}(\\Theta)$ and $J_{CV}(\\Theta)$ to be high with $J_{train}(\\Theta) \\approx J_{CV}(\\Theta)$.\n",
    "\n",
    "![Bias vs Variance](images\\bias_vs_variance1.png \"Bias vs Variance\")\n",
    "\n",
    "If a learning algorithm is suffering from high bias, getting more training data will not (by itself) help much.\n",
    "\n",
    "### Experiencing High Variance\n",
    "\n",
    "A low training set size: $J_{train}(\\Theta)$ will be low and $J_{CV}(\\Theta)$ will be high. A large training set size: $J_{train}(\\Theta)$ increases with training set size and $J_{CV}(\\Theta)$ continues to decrease without leveling off. Also, $J_{train}(\\Theta) \\lt J_{CV}(\\Theta)$ but the difference between them remains significant.\n",
    "\n",
    "![Bias vs Variance](images\\bias_vs_variance2.png \"Bias vs Variance\")\n",
    "\n",
    "If a learning algorithm is suffering from high variance, getting more training data is likely to help.\n",
    "\n",
    "## Examples\n",
    "\n",
    " * Get more training examples: fixes high variance.\n",
    " * Try smaller sets of features: fixes high variance.\n",
    " * Try getting additional features: fixes high bias.\n",
    " * Try adding polynomial features ($x_1^2, x_2^2, x_1x_2,$ etc.): fixes high bias.\n",
    " * Try decreasing $\\lambda$: fixes high bias.\n",
    " * Try increasing $\\lambda$: fixes high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binomial Distribution, Binomial Coefficient and Combinations\n",
    "---\n",
    "\n",
    "\n",
    "## Formula\n",
    "\n",
    "Binomial coefficient (equivalent notations):\n",
    "\n",
    "$\\large \\dbinom{n}{k} = C(n,k) = \\dfrac{n!}{k! \\, (n-k)!}$\n",
    "\n",
    "Binomial distribution formula:\n",
    "\n",
    "$\\large P(k) = \\dbinom{n}{k}\\,p^{k}\\,(1 - p)^{n-k}$\n",
    "\n",
    "In mathematics, a combination is a selection of items from a collection, such that the order of selection does not matter (unlike permutations). For example, given three fruits, say an apple, an orange and a pear, there are three combinations of two that can be drawn from this set: an apple and a pear; an apple and an orange; or a pear and an orange.\n",
    "\n",
    "The set of all $k$-combinations of a set $S$ is often denoted by $\\,S\\,\\choose\\,k\\,$. More formally, a $k$-combination of a set $S$ is a subset of $k$ distinct elements of $S$. If the set has $n$ elements, the number of $k$-combinations is equal to the binomial coefficient which can be written using factorials, whenever $k \\leq n$, and which is zero when $k > n$.\n",
    "\n",
    "The binomial coefficients occur in many areas of mathematics, and especially in combinatorics. The symbol $\\tbinom {n}{k}$ is usually read as \"$n$ choose $k$\" because there are $\\tbinom{n}{k}$ ways to choose an (unordered) subset of $k$ elements from a fixed set of $n$ elements.\n",
    "\n",
    "For example, there are $\\tbinom{\\,4\\,}{\\,2\\,} = 6$ ways to choose $2$ elements from $\\{1,2,3,4\\}$, namely $\\{1,2\\},\\,\\{1,3\\},\\,\\{1,4\\},\\,\\{2,3\\},\\,\\{2,4\\}$ and $\\{3,4\\}$.\n",
    "\n",
    "The binomial coefficients can be generalized to $\\tbinom{\\,z\\,}{\\,k\\,}$ for any complex number $z$ and integer $k \\geqslant 0$, and many of their properties continue to hold in this more general form.\n",
    "\n",
    "## Examples\n",
    "\n",
    "Calculate how many unique teams of $5$ people can be formed from $10$ people:\n",
    "\n",
    "$\\dbinom{n}{k} = \\dfrac{n!}{k! \\, (n - k)!} = \\dfrac{10!}{5! \\, (10! - 5!)} = \\dfrac{10!}{5! \\, 5!} = \\dfrac{10 \\, \\cdot \\, 9 \\, \\cdot \\, 8 \\, \\cdot \\, 7 \\, \\cdot \\, 6 \\, \\cdot \\, 5 \\, \\cdot \\, 4 \\, \\cdot \\, 3 \\, \\cdot \\, 2 \\, \\cdot \\, 1}{5 \\, \\cdot \\, 4 \\, \\cdot \\, 3 \\, \\cdot \\, 2 \\, \\cdot \\, 1} = 252$\n",
    "\n",
    "Calculate the probability of getting exactly $6$ heads in $10$ throws if a coin is bent so that it has a $40\\%$ probability of coming up heads:\n",
    "\n",
    "$P(6) = \\dbinom{10}{6}\\,0.4^{6}\\,0.6^{10-6} = 0.11147673600000005$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sanity check:  True\nPDF (Probability of exactly 8 events in 10 trials): 4.39% (0.0439453125)\nCDF (Probability of greater or equal to 8 events in 10 trials): 5.47% (0.0546875)\nCDF (Probability of less than 8 events in 10 trials): 94.53% (0.9453125)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Probability of exactly 8 events\t0.010616832000 (1.06%)\t\n",
    "Probability of X ≤ 8 events\t0.998322278400 (99.83%)\t\n",
    "Probability of X > 8 events\t0.001677721600 (0.17%)\t\n",
    "Probability of X < 8 events\t0.987705446400 (98.77%)\t\n",
    "Probability of X ≥ 8 events\t0.012294553600 (1.23%)\n",
    "\"\"\"\n",
    "import math\n",
    "import scipy.special\n",
    "\n",
    "# Calculate the probability of a bent coin with a p probability\n",
    "# of coming up heads, to get k heads in n throws:\n",
    "k = 8    # Number of target successful outcomes\n",
    "n = 10   # Number of trials to be performed\n",
    "p = 0.5  # Percentage of expected successful outcomes (in decimal notation)\n",
    "q = 1-p  # Percentage of expected unsucessful outcomes (in decimal notation)\n",
    "\n",
    "# Check if comb() and binom() return the same result\n",
    "# as doing the binomial calculation without them\n",
    "print(\"Sanity check: \", scipy.special.comb(n, k) == \n",
    "    scipy.special.binom(n, k) == (math.factorial(n) / \n",
    "    (math.factorial(k) * math.factorial(n-k))))\n",
    "\n",
    "# Use a binomial Probability Distribution Function (PDF) to calculate for\n",
    "# \"exactly k events\":\n",
    "# P(k) = (n!/k!(n-k)!) (p^k) (q^(n-k))\n",
    "pdf = scipy.special.comb(n, k) * (p**k) * (q**(n-k))\n",
    "print(f\"PDF (Probability of exactly {k} events in {n} trials): {round(pdf * 100, 2)}% ({pdf})\")\n",
    "\n",
    "cdf = 0\n",
    "for i in range(0, k):\n",
    "    cdf = cdf + scipy.special.comb(n, i) * (p**i) * (q**(n-i))\n",
    "\n",
    "# Use a binomial Cumulative Distribution Function (CDF) to calculate for \n",
    "# \"equal or greater than k events\":\n",
    "# P(k) = (1 - sum( (n!/k!(n-k)!) (p^k) (q^(n-k)) {0 >= k} ))\n",
    "print(f\"CDF (Probability of greater or equal to {k} events in {n} trials): {round((1 - cdf) * 100, 2)}% ({1 - cdf})\")\n",
    "\n",
    "# Use a binomial Cumulative Distribution Function (CDF) to calculate for \n",
    "# \"less than k events\":\n",
    "# P(k) = (sum( (n!/k!(n-k)!) (p^k) (q^(n-k)) {0 < k} ))\n",
    "print(f\"CDF (Probability of less than {k} events in {n} trials): {round((cdf) * 100, 2)}% ({cdf})\")"
   ]
  },
  {
   "source": [
    "# Binomial Theorem\n",
    "---\n",
    "In elementary algebra, the binomial theorem (or binomial expansion) describes the algebraic expansion of powers of a binomial. According to the theorem, it is possible to expand the polynomial $(x + y)^{n}$ into a sum involving terms of the form $ax^{b}y^{c}$, where the exponents $b$ and $c$ are non-negative integers with $b + c = n$, and the coefficient $a$ of each term is a specific positive integer depending on $n$ and $b$.\n",
    "\n",
    "Pascal's triangle determines the coefficients which arise in binomial expansions.\n",
    "\n",
    "![Pascal's Triangle](images\\pascal_triangle.png \"Pascal's Triangle\")\n",
    "\n",
    "## Examples\n",
    "\n",
    "Here are the first few cases of the binomial theorem:\n",
    "\n",
    "$(x+y)^0 = 1,\\\\\n",
    "(x+y)^1 = x + y,\\\\\n",
    "(x+y)^2 = x^2 + 2xy + y^2,\\\\\n",
    "(x+y)^3 = x^3 + 3x^2y + 3xy^2 + y^3,\\\\\n",
    "(x+y)^4 = x^4 + 4x^3y + 6x^2y^2 + 4xy^3 + y^4,\\\\\n",
    "(x+y)^5 = x^5 + 5x^4y + 10x^3y^2 + 10x^2y^3 + 5xy^4 + y^5,\\\\\n",
    "(x+y)^6 = x^6 + 6x^5y + 15x^4y^2 + 20x^3y^3 + 15x^2y^4 + 6xy^5 + y^6,\\\\\n",
    "(x+y)^7 = x^7 + 7x^6y + 21x^5y^2 + 35x^4y^3 + 35x^3y^4 + 21x^2y^5 + 7xy^6 + y^7,\\\\\n",
    "(x+y)^8 = x^8 + 8x^7y + 28x^6y^2 + 56x^5y^3 + 70x^4y^4 + 56x^3y^5 + 28x^2y^6 + 8xy^7 + y^8$\n",
    "\n",
    "So that:\n",
    "\n",
    "$(x + 1)^{2} = x^{2} + 2x + 1$\n",
    "\n",
    "$(b + 5)^{3} = 1b^{3} + 3b^{2}5 + 3b5^{2} + 1(5^{3}) = b^{3} + 15b^{2} + 75b + 125$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Chain Rule\n",
    "---\n",
    "## Formula\n",
    "\n",
    "$\\LARGE \\frac{d}{dx} \\Large \\left[ f(g(x)) \\right] = f'(g(x)) \\cdot g'(x)$\n",
    "\n",
    "The chain rule says that the derivative of the composite function is the inner function $g$ within the derivative of the outer function $f'$, multiplied by the derivative of the inner function $g'$.\n",
    "\n",
    "## Example\n",
    "\n",
    "$h$ is a composite function, where $g(x) = 5 - 6x$ is the inner function and $f(x) = x^{5}$ is the outer function:\n",
    "\n",
    "$h(x) = (5 - 6x)^{5}$\n",
    "\n",
    "First, we find the derivatives of the inner and outer functions:\n",
    "\n",
    "$g'(x) = -6$\n",
    "\n",
    "$f'(x) = 5x^{4}$\n",
    "\n",
    "Then, we apply the chain rule:\n",
    "\n",
    "$\\dfrac{d}{dx} \\left[ f(g(x)) \\right]$\n",
    "\n",
    "$= f'(g(x)) \\cdot g'(x)$\n",
    "\n",
    "$5(5 - 6x)^{4} \\cdot -6$\n",
    "\n",
    "$= -30(5 - 6x)^{4}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed-Form Expression\n",
    "---\n",
    "In mathematics, a closed-form expression is a mathematical expression expressed using a finite number of standard operations. It may contain constants, variables, certain \"well-known\" operations (e.g., + − × ÷), and functions (e.g., nth root, exponent, logarithm, trigonometric functions, and inverse hyperbolic functions), but usually no limit, differentiation, or integration.\n",
    "\n",
    "The set of operations and functions admitted in a closed-form expression may vary with author and context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coefficient of Determination\n",
    "---\n",
    "\n",
    "## Formula\n",
    "\n",
    "$\\Large R^{2} = 1 - \\large \\dfrac{TSS (Total \\, Sum \\, Of \\, Squares)}{RSS (Residual \\, Sum \\, Of \\, Squares)}$\n",
    "\n",
    "The coefficient of determination, also designated $R^{2}$ or R squared, is the proportion of the variance in the response variable that can be explained by the predictor variable. The coefficient of determination can range from 0 to 1. A value of 0 indicates that the response variable cannot be explained by the predictor variable at all. A value of 1 indicates that the response variable can be perfectly explained without error by the predictor variable.\n",
    "\n",
    "The Residual Sum of Squares (RSS), also known as the Sum of Squared Residuals (SSR) or the Sum of Squared Estimate of Errors (SSE), is the sum of the squares of residuals (deviations predicted from actual empirical values of data). It is a measure of the discrepancy between the data and an estimation model. A small RSS indicates a tight fit of the model to the data. It is used as an optimality criterion in parameter selection and model selection.\n",
    "\n",
    "## Examples\n",
    "\n",
    "TBD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compound Interest\n",
    "---\n",
    "\n",
    "## Formula\n",
    "\n",
    "$\n",
    "\\Large\n",
    "A = P\\left(1 + \\frac{r}{n}\\right)^{nt}\n",
    "$\n",
    "\n",
    "The concept of compound interest is that interest is added back to the principal sum so that interest is gained on that already accumulated interest, during the next compounding period, or in other words, interest on interest. It is the result of reinvesting interest, rather than paying it out, so that interest in the next period is then earned on the principal sum plus previously accumulated interest. Compound interest is standard in finance and economics.\n",
    "\n",
    "To use the compound interest formula, we need figures for principal amount, annual interest rate, time factor and the number of compound periods, so that:\n",
    "\n",
    "* $P$ is the principal investment amount (the initial deposit or loan amount).\n",
    "* $r$ is the annual interest rate (in decimal form).\n",
    "* $n$ is the number of times that interest is compounded per unit $t$.\n",
    "* $t$ is the time the money is invested or borrowed for.\n",
    "* $A$ is the future value of the investment/loan, including interest.\n",
    "\n",
    "## Examples\n",
    "\n",
    "Calculate the value of an investment of an amount of $5,000 deposited into a savings account after 10 years, at an annual interest rate of 5%, compounded monthly:\n",
    "\n",
    "$\\large A = 5000 \\, \\cdot \\, \\left(1 + \\dfrac{0.05}{12}\\right)^{(12 \\, \\cdot \\, 10)} = 8235.05$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "P = 5000  # Principal (initial investment)\n",
    "r = 0.05  # Annual interest rate\n",
    "n = 12    # Number of times to calculate interest within each time unit\n",
    "t = 10    # Total time of the investment\n",
    "\n",
    "total_investment_value = P * (1 + r/n) ** (n*t)\n",
    "round(total_investment_value, 2) == 8235.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Probability\n",
    "---\n",
    "\n",
    "In probability theory and statistics, given two jointly distributed random variables $X$ and $Y$, the conditional probability distribution of $Y$ given $X$ is the probability distribution of $Y$ when $X$ is known to be a particular value; in some cases the conditional probabilities may be expressed as functions containing the unspecified value $x$ of $X$ as a parameter. When both $X$ and $Y$ are categorical variables, a conditional probability table is typically used to represent the conditional probability. The conditional distribution contrasts with the marginal distribution of a random variable, which is its distribution without reference to the value of the other variable.\n",
    "\n",
    "If the conditional distribution of $Y$ given $X$ is a continuous distribution, then its probability density function is known as the \"conditional density function\". The properties of a conditional distribution, such as the moments, are often referred to by corresponding names such as the conditional mean and conditional variance.\n",
    "\n",
    "More generally, one can refer to the conditional distribution of a subset of a set of more than two variables; this conditional distribution is contingent on the values of all the remaining variables, and if more than one variable is included in the subset then this conditional distribution is the conditional joint distribution of the included variables.\n",
    "\n",
    "## Examples\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Compound Return\n",
    "---\n",
    "\n",
    "## Formula\n",
    "\n",
    "$\n",
    "\\large\n",
    "r_{continuous} = \\dfrac{\\ln \\dfrac{Value_{End}}{Value_{Start}}}{Periods}\n",
    "$\n",
    "\n",
    "A compound interest is interest calculated on the initial principal and also on the accumulated interest of previous periods of a deposit or loan. The effect of compound interest depends on frequency.\n",
    "\n",
    "Continuous compounding is the mathematical limit that compound interest can reach. It can be thought of as making the compounding period infinitesimally small, achieved by taking the limit as $n$ goes to infinity. It is an extreme case of compounding, since most interest is compounded on a monthly, quarterly or semiannual basis.\n",
    "\n",
    "As the number of compounding periods $n$ reaches infinity in continuous compounding, the continuous compound interest rate is referred to as the \"force of interest\" $\\delta$.\n",
    "\n",
    "![Compound interest graph](images\\compound_interest.png \"Compound interest graph\")\n",
    "\n",
    "## Examples\n",
    "\n",
    "Calculate the continuously compounded rate of return of a 1,600 investment which is worth 7,400 after 8.5 years:\n",
    "\n",
    "$\\large \\dfrac{\\ln \\dfrac{7400}{1600}}{8.5} = 0.18017 = 18\\%$\n",
    "\n",
    "<br>\n",
    "\n",
    "Calculate the total weight of a baby elephant of $200$Kg weight, after $3$ years, assuming a continuous growth of $5\\%$ per year:\n",
    "\n",
    "$\\large 200 \\cdot e^{(0.05)(3)} = 232.4$Kg\n",
    "\n",
    "<br>\n",
    "\n",
    "Convert an interest rate from a compounding basis to another compounding basis:\n",
    "\n",
    "$\\large r_{2} = \\left[\\left(1 + \\dfrac{r_{1}}{n_{1}}^\\frac{n1}{n2}\\right) - 1 \\right] n_{2}$\n",
    "\n",
    "<br>\n",
    "\n",
    "Convert from a continuously compounding interest to another compounding interest rate, where $\\delta$ is the interest rate on a continuous compounding basis, and $r$ is the stated interest rate, with a compounding frequency $n$:\n",
    "\n",
    "$\\delta = n \\ln\\left(1 + \\large\\frac{r}{n}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Calculate the continuously compounded rate of return of a\n",
    "# 1,600 investment which is worth 7,400 after 8.5 years\n",
    "math.log(7400/1600) / 8.5 == 0.18017369070169278"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "---\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "$\\Large J(\\theta_{0}, \\theta_{1}) = \\frac{1}{2m} \\sum\\limits_{i=1}^{m} (\\hat{y}_{i} - y_{i})^{2} = \\frac{1}{2m} \\sum\\limits_{i=1}^{m} (h_{\\theta}(x_{i}) - y_{i})^{2}$\n",
    "\n",
    "We can measure the accuracy of our linear regression hypothesis function ($h_{\\theta}$) by using a cost function ($J$). This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from $x$'s and the actual output $y$'s.\n",
    "\n",
    "To break it apart, it is $\\frac{1}{2}\\bar{x}$ where $\\bar{x}$ is the mean of the squares of $h_\\theta (x_{i}) - y_{i}$, or the difference between the predicted value and the actual value.\n",
    "\n",
    "This function is otherwise called the \"Squared Error Function\", or \"Mean Squared Error\". The mean is halved $\\left(\\frac{1}{2}\\right)$ as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the $\\frac{1}{2}$ term.\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "$\\Large J(\\theta) = \\frac{1}{m} \\sum\\limits_{i=1}^{m} Cost(h_{\\theta}(x_{i}), y_{i})$\n",
    "\n",
    "In logistic regression, the cost function can be expressed as:\n",
    "\n",
    "$\\large Cost(h_{\\theta}(x), y) = -\\log(h_{\\theta}(x)) \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, if \\, \\, \\, y = 1$\n",
    "\n",
    "$\\large Cost(h_{\\theta}(x), y) = -\\log(1 - h_{\\theta}(x)) \\, \\, \\, \\, \\, \\, \\ \\, \\, \\, \\, \\, if \\, \\, \\, y = 0$\n",
    "\n",
    "Or, using a more compressed version that fits in a single line:\n",
    "\n",
    "$\\large Cost(h_{\\theta}(x), y) = -y \\log(h_{\\theta}(x)) - ((1 - y) \\log(1 - h_{\\theta}(x))$\n",
    "\n",
    "So that the full logistic regression formula (including the compact version of its cost function) becomes:\n",
    "\n",
    "$\\Large J(\\theta) = - \\frac{1}{m} [ \\, \\sum\\limits_{i=1}^{m} y_{i} \\log h_{\\theta}(x_{i}) + (1 - y_{i}) \\log(1 - h_{\\theta}(x_{i})) \\, ]$\n",
    "\n",
    "Noting that we cannot use the same cost function that we use for linear regression because the Logistic Function would cause the output to be wavy, causing many local optima. In other words, it would not be a convex function.\n",
    "\n",
    "If our correct answer $y$ is $0$, then the cost function will be $0$ if our hypothesis function also outputs $0$. If our hypothesis approaches $1$, then the cost function will approach infinity.\n",
    "\n",
    "If our correct answer $y$ is $1$, then the cost function will be $0$ if our hypothesis function outputs $1$. If our hypothesis approaches $0$, then the cost function will approach infinity.\n",
    "\n",
    "Note that writing the cost function in this way guarantees that $J(\\theta)$ is convex for logistic regression.\n",
    "\n",
    "## Regularized Logistic Regression\n",
    "\n",
    "The cost function for regularized logistic regression is:\n",
    "\n",
    "$\\Large J(\\theta) = - \\frac{1}{m} \\sum\\limits_{i=1}^{m} [ \\, y^{(i)} \\log (h_{\\theta}(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)})) \\, ] + \\frac{\\lambda}{2m} \\sum\\limits_{j=1}^{n} \\theta_{j}^{2}$\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "The cost function for neural networks is a generalization of the cost function for logistic regression:\n",
    "\n",
    "$\\LARGE J(\\theta) = - \\frac{1}{m} \\sum\\limits_{i=1}^{m} \\sum\\limits_{k=1}^{K} [ \\, y_{k}^{(i)} \\log((h_{\\theta}(x^{(i)}))_{k}) + (1 - y_{k}^{(i)}) \\log(1 - (h_{\\theta}(x^{(i)}))_{k}) \\, ] + \\frac{\\lambda}{2m} \\sum\\limits_{l=1}^{L-1} \\sum\\limits_{i=1}^{s_{l}} \\sum\\limits_{j=1}^{s_{l+1}} (\\theta_{j, i}^{(l)})^{2}$\n",
    "\n",
    "## Examples\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "---\n",
    "Cross-validation is a technique in which we train our model using the subset of the data-set and then evaluate using the complementary subset of the data-set. Sometimes called \"rotation estimation\" or \"out-of-sample testing\", is one of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.\n",
    "\n",
    "In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set). The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).\n",
    "\n",
    "One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model's predictive performance.\n",
    "\n",
    "The three steps involved in cross-validation are as follows:\n",
    "\n",
    " * Reserve some portion of sample data-set.\n",
    " * Using the rest data-set train the model.\n",
    " * Test the model using the reserve portion of the data-set.\n",
    "\n",
    "## Methods Of Cross-Validation\n",
    "\n",
    "### Validation\n",
    "\n",
    "In this method, we perform training on the $50$% of the given dataset and rest $50$% is used for the testing purpose. The major drawback of this method is that we perform training on the 50% of the dataset, it may possible that the remaining $50$% of the data contains some important information which we are leaving while training our model i.e higher bias.\n",
    "\n",
    "### LOOCV (Leave One Out Cross Validation)\n",
    "\n",
    "In this method, we perform training on the whole dataset but leaves only one data point of the available dataset and then iterates for each data point. It has some advantages as well as disadvantages also.\n",
    "\n",
    "An advantage of using this method is that we make use of all data points and hence it is low bias.\n",
    "\n",
    "The major drawback of this method is that it leads to higher variation in the testing model as we are testing against one data point. If the data point is an outlier it can lead to higher variation. Another drawback is it takes a lot of execution time.\n",
    "\n",
    "### K-Fold Cross Validation\n",
    "\n",
    "In this method, we split the data-set into $k$ number of subsets(known as folds) then we perform training on the all the subsets but leave one($k-1$) subset for the evaluation of the trained model. In this method, we iterate $k$ times with a different subset reserved for testing purpose each time.\n",
    "\n",
    "In summary, cross-validation combines (averages) measures of fitness in prediction to derive a more accurate estimate of model prediction performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "---\n",
    "\n",
    "Deep Learning is a particular kind of machine learning that achieves great power and flexibility by learning to represent the world as nested hierarchy of concepts, with each concept defined in relation to simpler concepts, and more abstract representations computed in terms of less abstract ones.\n",
    "\n",
    "![Deep Learning](images\\deep_learning.png \"Deep Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative\n",
    "---\n",
    "\n",
    "## Notation\n",
    "\n",
    "Common notations to express derivatives:\n",
    "\n",
    "### Lagrange\n",
    "\n",
    "In Lagrange's notation, the derivative of $f$ is expressed as $f'$ (pronounced \"_$f$ prime_\").\n",
    "\n",
    "This notation is probably the most common when dealing with functions with a single variable. If, instead of a function, we have an equation such as $y = f(x)$, we can also write $y'$ to represent the derivative. This, however, is less common to do.\n",
    "\n",
    "### Leibniz\n",
    "\n",
    "In Leibniz's notation, the derivative of $f$ is expressed as $\\dfrac{d}{dx}f(x)$. When we have an equation such as $y = f(x)$, we can express the derivative as $\\dfrac{dy}{dx}$. Here, $\\dfrac{d}{dx}$ serves as an operator that indicates a differentiation with respect to $x$. This notation also allows us to directly express the derivative of an expression without using a function or a dependent variable. For example, the derivative of $x^2$ can be expressed as $\\dfrac{d}{dx}(x^2)$.\n",
    "\n",
    "This notation, while less comfortable than Lagrange's notation, becomes very useful when dealing with integral calculus, differential equations, and multivariable calculus.\n",
    "\n",
    "### Newton\n",
    "\n",
    "In Newton's notation, the derivative of $f$ is expressed as $\\dot{f}$ and the derivative of $y = f(x)$ is expressed as $\\dot{y}$.\n",
    "\n",
    "This notation is mostly common in Physics and other sciences where calculus is applied in a real-world context.\n",
    "\n",
    "<br>\n",
    "\n",
    "Derivatives are the result of performing a differentiation process upon a function or an expression. The derivative of a function of a real variable measures the sensitivity to change of the function value (output value) with respect to a change in its argument (input value).\n",
    "\n",
    "Derivatives are a fundamental tool of Calculus. For example, the derivative of the position of a moving object with respect to time is the object's velocity: it measures how quickly the position of the object changes when time advances.\n",
    "\n",
    "The derivative of a function of a single variable at a chosen input value, when it exists, is the slope of the tangent line to the graph of the function at that point. The tangent line is the best linear approximation of the function near that input value.\n",
    "\n",
    "For this reason, the derivative is often described as the \"instantaneous rate of change\", the ratio of the instantaneous change in the dependent variable (the $y$-axis on a graph) to that of the independent variable (the $x$-axis on a graph).\n",
    "\n",
    "The process of finding a derivative is called \"differentiation\". The reverse process is called \"antidifferentiation\". The fundamental theorem of calculus relates antidifferentiation with integration. Differentiation and integration constitute the two fundamental operations in single-variable calculus.\n",
    "\n",
    "In mathematics and computer algebra, automatic differentiation, also called algorithmic differentiation, computational differentiation, auto-differentiation, or simply _autodiff_, is a set of techniques to numerically evaluate the derivative of a function specified by a computer program.\n",
    "\n",
    "Automatic differentiation exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions ($exp$, $log$, $sin$, $cos$, etc.). By applying the chain rule (a formula to compute the derivative of a composite function) repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program.\n",
    "\n",
    "\n",
    "## Examples\n",
    "\n",
    "$\\large f(x) = x \\sin\\left(x^{2}\\right) + 1 \\left\\{-2 < x < 3\\right\\}$\n",
    "\n",
    "$\\large g(x) = \\frac{d}{dx}f\\left(x\\right)$\n",
    "\n",
    "$\\large A = \\left(a,\\ f\\left(a\\right)\\right)$\n",
    "\n",
    "$\\large y = g\\left(a\\right) \\left(x-a\\right) + f\\left(a\\right)$\n",
    "\n",
    "![Tangent function animation](images\\tangent_function_animation.gif \"Tangent function animation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance\n",
    "---\n",
    "\n",
    "## Formula\n",
    "\n",
    "$\n",
    "\\large\n",
    "d(A, B) = \\LARGE\\sqrt{\\large(\\Delta x)^{2} + (\\Delta y)^{2}}\n",
    "$\n",
    "\n",
    "The distance formula is a variant of the Pythagorean Theorem used in geometry. The distance from a point A to a point B is sometimes denoted as $|AB|$. In most cases, \"distance from A to B\" is interchangeable with \"distance from B to A\".\n",
    "\n",
    "In mathematics, the Euclidean distance or Euclidean metric is the \"ordinary\" straight-line distance between two points in Euclidean space. With this distance, Euclidean space becomes a metric space.\n",
    "\n",
    "## Examples\n",
    "\n",
    "Calculate the distance between point $A = (2, 2)$ and point $B = (-1, -2)$:\n",
    "\n",
    "$\n",
    "d(A, B) = \\sqrt{\\left(-1 - 2\\right)^{2} + \\left(-2 - 2\\right)^{2}} = 5\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "A = [2, 2]\n",
    "B = [-1, -2]\n",
    "\n",
    "# The math.hypot() function calculates the hypotenuse\n",
    "# of a right triangle, which is given by sqrt(a*a + b*b)\n",
    "d_AB = math.hypot(B[1] - A[1], B[0] - A[0])\n",
    "\n",
    "# Output is 5.0\n",
    "print(d_AB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Element Of\n",
    "---\n",
    "\n",
    "## Symbol\n",
    "\n",
    "$\n",
    "\\Huge\n",
    "\\in\n",
    "$\n",
    "\n",
    "The relation _is an element of_, also called \"set membership\", is denoted by the symbol $\\in$. Writing $\\large x \\in A$ means that \"$\\large x$ is an element of $A$\". Some equivalent expressions are \"$\\large x$ is a member of $A$\", \"$\\large x$ belongs to $A$\", \"$\\large x$ is in $A$\" and \"$\\large x$ lies in $A$\".\n",
    "\n",
    "The negation of set membership is denoted by the symbol $\\notin$.\n",
    "\n",
    "For the relation $\\in$, the _converse relation_ or _transpose_ $\\in^T$ may be written as $A \\ni \\, x$, meaning $A$ contains or includes $x$.\n",
    "\n",
    "## Examples\n",
    "\n",
    "If set $A = \\{1, \\, 2, \\, -3, \\, 7\\}$ then $2 \\in A$, $A \\ni 2$, $5 \\notin A$, and $A \\not\\ni 5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empty Set\n",
    "---\n",
    "\n",
    "## Symbol\n",
    "\n",
    "$\n",
    "\\huge\n",
    "\\varnothing\n",
    "$\n",
    "or\n",
    "$\n",
    "\\huge\n",
    "\\emptyset\n",
    "$\n",
    "or\n",
    "$\n",
    "\\LARGE\n",
    "\\{\\}\n",
    "$\n",
    "\n",
    "In mathematics, the empty set is the unique set having no elements. Its size or cardinality (count of elements in a set) is zero. Some axiomatic set theories ensure that the empty set exists by including an axiom of empty set, while in other theories, its existence can be deduced. Many possible properties of sets are vacuously true for the empty set.\n",
    "\n",
    "In some textbooks and popularizations, the empty set is referred to as the \"null set\". However, null set is a distinct notion within the context of measure theory, in which it describes a set of measure zero (which is not necessarily empty). The empty set may also be called the \"void set\".\n",
    "\n",
    "It is important not to confuse $\\emptyset$ with $\\{\\emptyset\\}$. The former has no elements, while the latter has one element. If we visualize the empty set as an empty paper bag, then we can visualize $\\{\\emptyset\\}$ as a paper bag inside a paper bag.\n",
    "\n",
    "## Examples\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epoch\n",
    "---\n",
    "\n",
    "Number epoch equal to the number of times the algorithm sees the entire data set. So, each time the algorithm has seen all samples in the dataset, one epoch has completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Euler's Number\n",
    "---\n",
    "\n",
    "## Symbol\n",
    "\n",
    "$\n",
    "\\Huge\n",
    "e\n",
    "$\n",
    "\n",
    "The number $e$, also known as \"Euler's number\", is a mathematical constant approximately equal to $2.718281828459045$, and can be characterized in many ways. It is the base of the natural logarithm. It is the limit of $(1 + 1/n)^{n}$ as $n$ approaches infinity, an expression that arises in the study of compound interest. It can also be calculated as the sum of the infinite series:\n",
    "\n",
    "$\\large e = \\sum\\limits_{n=0}^{\\infty}\\frac{1}{n!} = \\frac{1}{1} + \\frac{1}{1} + \\frac{1}{1 \\, \\cdot \\, 2} + \\frac{1}{1 \\, \\cdot \\, 2 \\, \\cdot \\, 3} \\, + \\, \\cdots \\,$\n",
    "\n",
    "It is also the unique positive number $a$ such that the graph of the function $y = ax$ has unit slope at $x = 0$.\n",
    "\n",
    "The (natural) exponential function $f(x) = e^{x}$ is the unique function which is equal to its own derivative, with the initial value $f(0) = 1$ (and hence one may define $e$ as $f(1)$).\n",
    "\n",
    "The natural logarithm, or logarithm to base $e$, is the inverse function to the natural exponential function. The natural logarithm of a number $k > 1$ can be defined directly as the area under the curve $y = 1/x$ between $x = 1$ and $x = k$, in which case $e$ is the value of $k$ for which this area equals one:\n",
    "\n",
    "![Graph of the equation y = 1/x](images\\eulers_number.png \"Graph of the equation y = 1/x\")\n",
    "\n",
    "The discovery of the constant itself is credited to Jacob Bernoulli in 1683, who attempted to find the value of the following expression (which is equal to $e$):\n",
    "\n",
    "$\\large \\lim\\limits_{n\\to\\infty} \\left( 1 + \\frac{1}{n} \\right)^n$\n",
    "\n",
    "## Examples\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event\n",
    "---\n",
    "\n",
    "In probability theory, an event is a set of possible outcomes of an experiment (a subset of the sample space) to which a probability is assigned. Any subset of $S$ is an event. A single outcome may be an element of many different events, and different events in an experiment are usually not equally likely, since they may include very different groups of outcomes.\n",
    "\n",
    "An event defines a complementary event, namely the complementary set (the event not occurring), and together these define a Bernoulli trial: did the event occur or not?\n",
    "\n",
    "Typically, when the sample space is finite, any subset of the sample space is an event (i.e. all elements of the power set of the sample space are defined as events). However, this approach does not work well in cases where the sample space is uncountably infinite. So, when defining a probability space, it is possible, and often necessary, to exclude certain subsets of the sample space from being events.\n",
    "\n",
    "## Examples\n",
    "\n",
    "Considering $E_{1}$ as the experiment of tossing a die and getting an even number event, then $A_{1} = \\{2, 4, 6\\}$, and $A'_{1} = \\{1, 3, 5\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration\n",
    "---\n",
    "\n",
    "Every time a batch of data passes through the neural network, completes one iteration. In the case of neural networks, that means the forward pass and backward pass. So, epoch = batch size * number of iterations. So, one epoch includes all the training examples whereas one iteration includes only one batch of training examples.\n",
    "\n",
    "One training step is equal to process one batch of data, while all batches need to be processed to make one epoch. Steps parameter indicates the number of steps to run over data. A training step corresponds to one gradient update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorial\n",
    "---\n",
    "\n",
    "## Formula\n",
    "\n",
    "$\\Large n! = \\prod\\limits_{i = 1}^{n} i \\,\\,$\n",
    "or\n",
    "$\\Large \\,\\, n! =\\large n \\cdot (n - 1) \\cdot (n - 2) \\cdot (n - 3) \\cdot \\, \\dots \\, 3 \\cdot 2 \\cdot 1$\n",
    "\n",
    "In mathematics, the factorial of a positive integer $n$, denoted by $n!$, is the product of all positive integers less than or equal to $n$. The value of $0!$ is $1$, according to the convention for an empty product.\n",
    "\n",
    "The factorial operation is encountered in many areas of mathematics, notably in combinatorics, algebra, and mathematical analysis. Its most basic use counts the possible distinct sequences – the permutations – of $n$ distinct objects: there are $n!$.\n",
    "\n",
    "The factorial function can also be extended to non-integer arguments while retaining its most important properties by defining $x! = \\Gamma(x + 1)$, where $\\Gamma$ is the gamma function; this is _undefined_ when $x$ is a negative integer.\n",
    "\n",
    "## Examples\n",
    "\n",
    "$\\large 5! = 5 \\cdot 4 \\cdot 3 \\cdot 2 \\cdot 1 = 120$\n",
    "\n",
    "$\\Large \\frac{7!}{5!} = \\frac{7 \\, \\cdot \\, 6 \\, \\cdot \\, 5 \\, \\cdot \\, 4 \\, \\cdot \\, 3 \\, \\cdot \\, 2 \\, \\cdot \\, 1}{5 \\, \\cdot \\, 4 \\, \\cdot \\, 3 \\, \\cdot \\, 2 \\, \\cdot \\, 1}\\large = 42$\n",
    "\n",
    "$\\large 0! = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "math.factorial(5) == 120"
   ]
  },
  {
   "source": [
    "# Gaussian Distribution\n",
    "---\n",
    "## Original Model\n",
    "\n",
    "$\\Large p(x_{1}; \\mu_{1}, \\sigma_{1}^{2}) \\times \\cdots \\times p(x_{1}; \\mu_{n}, \\sigma_{n}^{2})$\n",
    "\n",
    "The original model can work fine if we can manually create the features we need to capture anomalies where $x_{1}, x_{2}$ take unusual combinations of values. Also, compared to the multivariate model, it is computationally cheaper, therefore it will scale better when we have a large $n$. The original model will also perform well even if $m$ (the training set size) is small.\n",
    "\n",
    "## Multivariate Gaussian Distribution\n",
    "\n",
    "$\\Large p(x; \\mu, \\Sigma) = \\dfrac{1}{(2\\pi) \\frac{n}{2} \\vert \\Sigma \\vert \\frac{1}{2}} \\, \\exp \\left( - \\dfrac{1}{2} (x - \\mu)^{T} \\Sigma^{-1}(x - \\mu)\\right)$\n",
    "\n",
    "Automatically captures correlations between features, however, it is computationally more expensive. Also, the multivariate model must have $m \\gt n$, otherwise $\\Sigma$ will be non-invertible. As a reference, we should have about ten times more observations $(m)$ than features $(n)$.\n",
    "\n",
    "### Mean ($\\mu$) Parameter\n",
    "\n",
    "$\\Large \\mu = \\frac{1}{m} \\sum\\limits_{i=1}^{m} x^{(i)}$\n",
    "\n",
    "### Covariance ($\\Sigma$) Parameter\n",
    "\n",
    "$\\Large \\Sigma = \\frac{1}{m} \\sum\\limits_{i=i}^{m} (x^{(i)} - \\mu)(x^{(i)} - \\mu)^{T}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Gaussian Function\n",
    "---\n",
    "## Formula\n",
    "\n",
    "$\\LARGE f(x) = a \\cdot \\exp \\left( - \\frac{(x - b)^{2}}{2c^{2}} \\right)$\n",
    "\n",
    "In mathematics, a Gaussian function, often simply referred to as a Gaussian, is a function for arbitrary real constants $a$, $b$ and non-zero $c$. It is named after the mathematician Carl Friedrich Gauss. The graph of a Gaussian is a characteristic symmetric \"bell curve\" shape. The parameter $a$ is the height of the curve's peak, $b$ is the position of the center of the peak and $c$ (the standard deviation, sometimes called the Gaussian RMS width) controls the width of the \"bell\".\n",
    "\n",
    "Gaussian functions are often used to represent the probability density function of a normally distributed random variable with expected value $\\mu = b$ and variance $\\sigma^{2} = c^{2}$.\n",
    "\n",
    "In this case, the Gaussian is of the form:\n",
    "\n",
    "$\\Large g(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\, \\, \\exp \\left( - \\frac{1}{2} \\frac{(x - \\mu)^{2}}{\\sigma^{2}} \\right)$\n",
    "\n",
    "![Normalized Gaussian Curves](images\\normalized_gaussian_curves.png \"Normalized Gaussian Curves\")\n",
    "\n",
    "Gaussian functions are widely used in statistics to describe the normal distributions, in signal processing to define Gaussian filters, in image processing where two-dimensional Gaussians are used for Gaussian blurs, and in mathematics to solve heat equations and diffusion equations and to define the Weierstrass transform."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Gaussian Kernel\n",
    "---\n",
    "## Formula\n",
    "\n",
    "$\\Large K_{gaussian}(x^{(i)}, x^{(j)}) = \\exp \\left( - \\dfrac{\\parallel x^{(i)} - x^{(j)} \\parallel^{2}}{2 \\sigma^{2}} \\right) = \\exp\\left( - \\dfrac{\\sum\\limits_{k=1}^{n} (x_{k}^{(i)} - x_{k}^{(j)})^{2}}{2 \\sigma^{2}} \\right)$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "---\n",
    "\n",
    "## Formula\n",
    "\n",
    "The general form of gradient descent is:\n",
    "\n",
    "$\\LARGE \\theta_{j} := \\theta_{j} - \\alpha \\frac{\\partial}{\\partial\\theta_{j}} J(\\theta_{0}, \\theta_{1})$\n",
    "\n",
    "Using Calculus, we can work out the derivative part to get:\n",
    "\n",
    "$\\large Repeat \\, \\, \\{ \\\\\n",
    "    \\LARGE \\, \\, \\, \\, \\, \\, \\, \\theta_{j} := \\theta_{j} - \\frac{\\alpha}{m} \\sum\\limits_{i=1}^{m} (h_{\\theta}(x_{i}) - y_{i}) x_{j}\\\\\n",
    "\\large \\}$\n",
    "\n",
    "The goal of gradient descent is usually to minimize the loss function for a machine learning problem. A good algorithm finds the minimum fast and reliably well (i.e. it doesn’t get stuck in local minima, saddle points, or plateau regions, but rather goes for the global minimum). \n",
    "\n",
    "In the formula above, $\\alpha$ is the learning rate (it will be used as the step size of the gradient descendant). If $\\alpha$ is too small, the gradient descent will be slow. If $\\alpha$ is too large, the gradient descent can overshoot (miss or pass over) the minimum (it may fail to converge, or even diverge). It is also important to note that the gradient descent can converge to a local minimum even having the learning rate $\\alpha$ as a fixed value, because as it approaches the local minimum, it will automatically take smaller steps, so there is no need to decrease $\\alpha$ over time.\n",
    "\n",
    "The $\\frac{\\partial}{\\partial\\theta_{j}} J(\\theta_{0}, \\theta_{1})$ part of the formula is just the derivative (the slope). Regardless of the slope's sign, the derivative eventually converges to its minimum value, therefore, by definition, will return $0$ when the gradient descent reaches the local minimum. When the slope is negative, the value of $\\theta_{1}$ increases and when it is positive, the value of $\\theta_{1}$ decreases.\n",
    "\n",
    "![Gradient Descent Algorithm](images\\gradient_descent.gif \"Gradient Descent Algorithm\")\n",
    "\n",
    "The basic gradient descent algorithm follows the idea that the opposite direction of the gradient points to where the lower area is. So it iteratively takes steps in the opposite directions of the gradients.\n",
    "\n",
    "As human perception is limited to 3 dimensions, in all our visualizations, imagine we only have two parameters (or thetas) to optimize, and they are represented by the $x$ and $y$ dimensions in the graph. The surface is the loss function. We want to find the ($x$, $y$) combination that’s at the lowest point of the surface. The problem is trivial to us because we can see the whole surface. But the ball (the descent algorithm) doesn’t; it can only take one step at a time and explore its surroundings, analogous to walking in the dark with only a flashlight.\n",
    "\n",
    "![Gradient Descent Algorithm Comparison](images\\gradient_descent_2.png \"Gradient Descent Algorithm Comparison\")\n",
    "\n",
    "A batch gradient descent means that at each step of gradient descent all the training samples will be used.\n",
    "\n",
    "![Gradient Descent Algorithm Comparison](images\\gradient_descent_1.jpeg \"Gradient Descent Algorithm Comparison\")\n",
    "\n",
    "## Feature Scaling\n",
    "\n",
    "We can speed up gradient descent by having each of our input values in roughly the same range. This is because $\\theta$ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\n",
    "\n",
    "The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally:\n",
    "\n",
    "$-1 \\leq x_{(i)} \\leq 0.5$\n",
    "\n",
    "These aren't exact requirements; we are only trying to speed things up. The goal is to get all input variables into roughly one of these ranges, give or take a few.\n",
    "\n",
    "Two techniques to help with this are feature scaling and mean normalization. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula:\n",
    "\n",
    "$\\large x_{i} := \\dfrac{x_{i} - \\mu_{i}}{s_{i}}$\n",
    "\n",
    "Where $\\mu_{i}$ is the average of all the values for feature ($i$) and $s_{i}$ is the range of values (max - min), or $s_{i}$ is the standard deviation.\n",
    "\n",
    "Note that dividing by the range, or dividing by the standard deviation, give different results. For example, if $x_{i}$ represents housing prices with a range of $100$ to $2000$ and a mean value of $1000$, then, $x_{i} := \\dfrac{price-1000}{1900}$.\n",
    "\n",
    "## Learning Rate\n",
    "\n",
    " * Debugging gradient descent: make a plot with number of iterations on the $x$-axis. Now plot the cost function, $J(\\theta)$ over the number of iterations of gradient descent. If $J(\\theta)$ ever increases, then you probably need to decrease the learning rate ($\\alpha$).\n",
    "\n",
    " * Automatic convergence test: declare convergence if $J(\\theta)$ decreases by less than $E$ in one iteration, where $E$ is some small value such as $10^{−3}$. However, in practice it's difficult to choose this threshold value.\n",
    "\n",
    "It has been proven that if learning rate $\\alpha$ is sufficiently small, then $J(\\theta)$ will decrease on every iteration. To summarize:\n",
    "\n",
    " * If $\\alpha$ is too small: slow convergence. \n",
    " * If $\\alpha$ is too large: $J(\\theta)$ may not decrease on every iteration and thus may not converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace's Rule\n",
    "---\n",
    "\n",
    "## Formula\n",
    "\n",
    "$\\Large P(A) = \\dfrac{m}{n}$\n",
    "\n",
    "Laplace's rule allows to compute the probability of an event, always that the elementary events are equiprobable, that is, that all possible outcomes have the same probability. Under these conditions, the probability of an event is obtained by dividing $m$, the number of results that form the event (favorable cases), by $n$, the number of possible outcomes (possible cases). However, it is important to note that this rule only works when all cases are equiprobable.\n",
    "\n",
    "## Examples\n",
    "\n",
    " 1. If a family has two children, and we assume that the probability of being a man is the same as that of being a woman, what is the probability of having both children of the same sex?\n",
    "\n",
    " As we are going to apply Laplace's rule, we will consider the results in order. So, in this case, the favorable cases are $MM$ and $WW$, and our sample space is $\\Omega = \\{MM, MW, WM, WW\\}$.\n",
    "  \n",
    " Therefore, the probability is:\n",
    "\n",
    " $P(A) = \\dfrac{2}{4} = \\dfrac{1}{2} = 0.5 = 50\\%$\n",
    "\n",
    " 2. What is the probability of throwing two coins and getting heads both times?\n",
    " \n",
    " Every time you throw a coin it is equally probable to come out with heads or tails, so that all possible outcomes are equiprobable. \n",
    " \n",
    " Our sample space has four elements $\\Omega = \\{HH, HT, TH, TT\\}$, and there is only one case in favor of event \"get heads twice\".\n",
    " \n",
    " Therefore, the probability is:\n",
    "\n",
    " $P(A) = \\dfrac{1}{4} = 0.25 = 25\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space\n",
    "---\n",
    "\n",
    "The concept of “latent space” is important because it’s utility is at the core of Deep Learning - learning the features of data and simplifying data representations for the purpose of finding patterns. If it seems that this process is \"hidden\" it’s because it is. Latent, by definition, means \"hidden\".\n",
    "\n",
    "For example, let's say we train a model to classify an image using a fully convolutional neural network. This could be to output a digit number when given an image of a digit. As the model \"learns\", it is simply learning features at each layer (edges, angles, etc.) and attributing a combination of features to a specific output. Each time the model learns through a data point, the dimensionality of the image is first reduced before it is ultimately increased (through an Encoder and a Bottleneck).\n",
    "\n",
    "![Latent Space Representation](images\\latent_space.png \"Latent Space Representaion\")\n",
    "\n",
    "When the dimensionality is reduced, it is considered a form of lossy compression. That said, data is often compressed in machine learning to learn important information about data points. This compressed state is the \"Latent Space Representation\" of the data. Because the model is required to then reconstruct the compressed data (through a Decoder), it must learn to store all relevant information and disregard the noise. This is the value of compression - it allows us to get rid of any extraneous information, and only focus on the most important features.\n",
    "\n",
    "In other words, the model learns the data features and simplifies its representation to make it easier to analyze.\n",
    "This is at the core of a concept called Representation Learning, defined as a set of techniques that allow a system to discover the representations needed for feature detection or classification from raw data.\n",
    "\n",
    "The latent space is an essential concept in manifold learning, a subfield of representation learning. Manifolds in data science can be understood as groups or subsets of data that are \"similar\" in some way. These similarities, usually imperceptible or obscured in higher-dimensional space, can be discovered once data has been represented in the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit\n",
    "---\n",
    "\n",
    "## Symbol\n",
    "\n",
    "$\n",
    "\\LARGE\n",
    "\\lim\\limits_{x \\to \\infty} \\Large f(x)\n",
    "$\n",
    "\n",
    "There's an important difference between the value a function is approaching (what we call the limit) and the value of the function itself. In the graph below, as we get closer and closer to $x = 2$ from both the left and the right, we seem to approach $y = 0.25$. We see that the function value is _undefined_, but the limit value is approximately $0.25$.\n",
    "\n",
    "![Function's limit graph](images\\limit_in_graph.gif \"Limit of a function in a graph animation\")\n",
    "\n",
    "It is also possible for the function value to be different from the function limit value. In the next graph, the limit seems to be somewhere between $y = 4$ and $y = 5$, but slightly closer to $y = 4$.\n",
    "\n",
    "![Limit of a function can be different from its value graph](images\\limit_of_function_different_from_value.png \"Limit of a function can be different from its value in a graph example\")\n",
    "\n",
    "Just because a function is _undefined_ for some $x$-value, doesn't mean there is no limit. Holes in graphs happen with rational functions, which become _undefined_ when their denominators are zero. The graph of $y = x / sin(x)$ is a classic example. Notice there is a hole at $x = 0$ because the function is _undefined_ there. So, the function value is irrelevant to find the limit. All that matters is figuring out what the $y$-values are approaching as we get closer and closer:\n",
    "\n",
    "![A undefined function still has a limit](images\\undefined_function_still_has_limit.png \"A undefined function still has a limit graph example\")\n",
    "\n",
    "When the function is defined for some $x$-value, that doesn't mean that the limit necessarily exists. The following graph shows something that can happen when we're working with piecewise functions. Notice how we're not approaching the same $y$-value from both sides of $x = 3$:\n",
    "\n",
    "![A function limit that does not exists](images\\limit_does_not_exists.png \"A function limit that does not exists graph example\")\n",
    "\n",
    "A graph can help us to approximate a limit by allowing to estimate the finite $y$-value we are approaching as we get closer and closer to some $x$-value (from both sides). But even using a graphing calculator, we can only zoom in so far. We can't get infinitely close to the $x$-value we're interested in. The best we can do is reason about what the $y$-values seem to be approaching as our $x$-values get closer and closer to some number.\n",
    "\n",
    "## Examples\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Least Squares\n",
    "---\n",
    "\n",
    "Linear Least Squares (LLS) is the least squares approximation of linear functions to data. It is a set of formulations for solving statistical problems involved in linear regression, including variants for ordinary (unweighted), weighted, and generalized (correlated) residuals. Numerical methods for linear least squares include inverting the matrix of the normal equations and orthogonal decomposition methods.\n",
    "\n",
    "The three main linear least squares formulations are:\n",
    "\n",
    " * Ordinary Least Squares\n",
    " * Weighted Least Squares\n",
    " * Generalized Least Squares\n",
    "\n",
    "TBD\n",
    "\n",
    "## Examples\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "---\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "$\\LARGE\n",
    "h_{\\theta}(x) = \\sum\\limits_{j = 0}^{n} \\theta_{j} x_{j}\n",
    "$\n",
    "\n",
    "## Linear Regression with Regularization\n",
    "\n",
    "$\\LARGE\n",
    "J(\\theta) = \\frac{1}{2m} \\left[ \\sum\\limits_{i = 1}^{m} (h\\theta(x^{(i)}) − y^{(i)})^{2} + \\lambda \\sum\\limits_{j = 1}^{m} \\theta_{j}^{2} \\right]\n",
    "$\n",
    "\n",
    "## Linear Regression with Gradient Descent\n",
    "\n",
    "$\\large Repeat \\{\\\\\\LARGE\n",
    "\\theta_{j} := \\theta_{j} − \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h\\theta(x^{(j)}) − y^{(i)}) x_{j}^{(i)}\\\\\n",
    "\\large \\}$\n",
    "\n",
    "## Linear Regression with Gradient Descent and Regularization\n",
    "\n",
    "$\\large Repeat \\{\\\\\\LARGE\n",
    "\\, \\, \\, \\, \\, \\, \\theta_{0} := \\theta_{0} − \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h\\theta(x^{(i)}) − y^{(i)}) x_{0}^{(i)}\\\\\\LARGE\n",
    "\\, \\, \\, \\, \\, \\, \\theta_{j} := \\theta_{j} − \\alpha \\left[ \\left( \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h\\theta(x^{(i)}) − y^{(i)}) x_{j}^{(i)} \\right) + \\frac{\\lambda}{m} \\theta_{j} \\right] \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, j \\in \\{ 1, 2, \\dots n \\}\\\\\n",
    "\\large \\}$\n",
    "\n",
    "The term $\\frac{\\lambda}{m}\\theta_{j}$ performs the regularization. With some manipulation the update rule can also be represented as:\n",
    "\n",
    "$\\LARGE\n",
    "\\theta_{j} := \\theta_{j} (1 - \\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h\\theta(x^{(i)}) − y^{(i)}) x_{j}^{(i)}\n",
    "$\n",
    "\n",
    "Noting that, in the case of Logistic Regression, the Cost Function is:\n",
    "\n",
    "$\\LARGE h_{\\theta}(x) = \\frac{1}{1 \\, + \\, e^{-\\theta^{T}x}}$\n",
    "\n",
    "The Cost Function of Logistic Regression derivative can be computed as this:\n",
    "\n",
    "$\\LARGE -\\frac{y}{\\sigma(z)} + \\frac{1 - y}{1 - \\sigma(z)}$\n",
    "\n",
    "## Normal Equation\n",
    "\n",
    "$\\LARGE\n",
    "\\theta = (X^{T} X)^{-1} X^{T} y\n",
    "$\n",
    "\n",
    "## Normal Equation with Regularization\n",
    "\n",
    "$\\LARGE\n",
    "\\theta = (X^{T} X + \\lambda \\cdot L)^{-1} X^{T} y\n",
    "$\n",
    "\n",
    "$L$ is a matrix with $0$ at the top left and $1$'s down the diagonal, with $0$'s everywhere else. It should have dimension $(n+1) \\times (n+1)$. Intuitively, this is the identity matrix (though we are not including $x_{0}$), multiplied with a single real number $\\lambda$.\n",
    "\n",
    "Recall that if $m < n$, then $X^{T}X$ is non-invertible. However, when we add the term $\\lambda \\cdot L$, then $X^{T} X + \\lambda \\cdot L$ becomes invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marginal Distribution (Probability)\n",
    "---\n",
    "\n",
    "In probability theory and statistics, the marginal distribution of a subset of a collection of random variables is the probability distribution of the variables contained in the subset. It gives the probabilities of various values of the variables in the subset without reference to the values of the other variables. This contrasts with a conditional distribution, which gives the probabilities contingent upon the values of the other variables.\n",
    "\n",
    "Marginal variables are those variables in the subset of variables being retained. These concepts are \"marginal\" because they can be found by summing values in a table along rows or columns, and writing the sum in the margins of the table. The distribution of the marginal variables (the marginal distribution) is obtained by marginalizing – that is, focusing on the sums in the margin – over the distribution of the variables being discarded, and the discarded variables are said to have been marginalized out.\n",
    "\n",
    "The context here is that the theoretical studies being undertaken, or the data analysis being done, involves a wider set of random variables but that attention is being limited to a reduced number of those variables. In many applications, an analysis may start with a given collection of random variables, then first extend the set by defining new ones (such as the sum of the original random variables) and finally reduce the number by placing interest in the marginal distribution of a subset (such as the sum). Several different analyses may be done, each treating a different subset of variables as the marginal variables.\n",
    "\n",
    "## Examples\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chain\n",
    "---\n",
    "\n",
    "A Markov Chain is a probabilistic model used to estimate a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. In a Markov chain, the future state depends only on the present state and not on the past states.\n",
    "\n",
    "## Example\n",
    "\n",
    "An example of Markov’s process is the position of a randomly walking person when at instant $t+1$ would be dependent on the current state $t$ and not on the previous states ($t-1$, $t-2$, $\\cdots$). This behavior is referred to as Markov's property.\n",
    "\n",
    "![Markov Chain](images\\markov_chain.png \"Markov Chain\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation\n",
    "---\n",
    "In statistics, Maximum Likelihood Estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate. The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference.\n",
    "\n",
    "If the likelihood function is differentiable, the derivative test for determining maxima can be applied. In some cases, the first-order conditions of the likelihood function can be solved explicitly; for instance, the ordinary least squares estimator maximizes the likelihood of the linear regression model. Under most circumstances, however, numerical methods will be necessary to find the maximum of the likelihood function.\n",
    "\n",
    "From the vantage point of Bayesian inference, MLE is a special case of Maximum a Posteriori Estimation (MAP) that assumes a uniform prior distribution of the parameters. In frequentist inference, MLE is a special case of an extremum estimator, with the objective function being the likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean\n",
    "---\n",
    "\n",
    "## Symbol\n",
    "\n",
    "$\n",
    "\\Huge\n",
    "\\mu\n",
    "$\n",
    "or\n",
    "$\n",
    "\\Huge\n",
    "\\bar{x}\n",
    "$\n",
    "\n",
    "In statistics, the term _average_ refers to any of the measures of central tendency. The mean is the most commonly used and readily understood measure of central tendency in a data set.\n",
    "\n",
    "To express a population mean, use the $\\Large\\mu$ or $\\Large\\mu_{x}$ symbols. To express the arithmetic mean of a sample of the population, use a $\\Large\\bar{x}$ symbol.\n",
    "\n",
    "The arithmetic mean of a set of observed data is defined as being equal to the sum of the numerical values of each and every observation, divided by the total number of observations.\n",
    "\n",
    "Symbolically, if we have a data set consisting of the values $a_1, \\, a_2, \\, \\ldots, \\, a_n$, then the arithmetic mean $\\bar{x}$ is defined by the formula:\n",
    "\n",
    "$\\Large \\bar{x} = \\frac{1}{n}\\sum\\limits_{i=1}^n x_i  =  \\frac{1}{n} (x_1 + \\cdots + x_n)$\n",
    "\n",
    "## Examples\n",
    "\n",
    "The arithmetic mean of Z set is 6:\n",
    "\n",
    "If\n",
    "$\n",
    "Z = \\{1, 5, 12\\}\n",
    "$\n",
    "\n",
    "Then\n",
    "$\n",
    "\\Large\n",
    "\\mu_{z} = \\frac{1 + 5 + 12}{3} = \\frac{18}{3}\n",
    "= \\large 6\n",
    "$<br>\n",
    "Or (using summation notation)\n",
    "$\n",
    "\\Large\n",
    "\\mu_{z}\n",
    "= \\frac{1}{n}\n",
    "(\\sum\\limits_{i=1}^{n}z_{i})\n",
    "= \\large 6\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Squared Error\n",
    "---\n",
    "\n",
    "## Formula\n",
    "\n",
    "$\\Large MSE = \\frac{1}{n} \\sum\\limits_{i=1}^{n} (y_{i} - \\hat{f}(x_{i}))^2$\n",
    "\n",
    "In statistics, the Mean Squared Error (MSE) or Mean Squared Deviation (MSD) of an estimator (a procedure for estimating an unobserved quantity) measures the average of the squares of the errors (the average squared difference between the estimated values and the actual value).\n",
    "\n",
    "MSE is a risk function, corresponding to the expected value of the squared error loss. The fact that MSE is almost always strictly positive (and not zero) is because of randomness or because the estimator does not account for information that could produce a more accurate estimate.\n",
    "\n",
    "The MSE is a measure of the quality of an estimator, it is always non-negative, and values closer to zero are better.\n",
    "\n",
    "The MSE is the second moment (about the origin) of the error, and thus incorporates both the variance of the estimator (how widely spread the estimates are from one data sample to another) and its bias (how far off the average estimated value is from the true value). For an unbiased estimator, the MSE is the variance of the estimator. Like the variance, MSE has the same units of measurement as the square of the quantity being estimated.\n",
    "\n",
    "In an analogy to standard deviation, taking the square root of MSE yields the Root Mean Squared Error or Root Mean Squared Deviation (RMSE or RMSD), which has the same units as the quantity being estimated. For an unbiased estimator, the RMSE is the square root of the variance, known as the standard error.\n",
    "\n",
    "## Examples\n",
    "\n",
    "Considering a set of actual and predicted values, compute the respective squared error by squaring the difference between each predicted value ($\\hat{y}$) and its actual (observed) value ($y$).\n",
    "\n",
    "For example, for row $1$ in the table below, calculate $(28 - 19)^2 = 9^2 = 81$.\n",
    "\n",
    "![Mean Squared Error input data](images\\mean_squared_error.png \"Mean Squared Error input data\")\n",
    "\n",
    "To get the MSE of the whole dataset, calculate the squared error for each row, sum all of those, and divide the sum's total by the number of rows:\n",
    "\n",
    "$\\large \\dfrac{81 + 16 + 25 + 49 + 49}{5} = 44$\n",
    "\n",
    "The Mean Absolute Error (MAE) is similar to the MSE, however, it does not sum the square the errors of each result. Instead, it sums their absolute values:\n",
    "\n",
    "$\\large MAE = \\LARGE \\frac{\\sum\\limits_{i=1}^{n} |y_{i} - x_{i}|}{n} = \\frac{\\sum\\limits_{i=1}^{n} |e_{i}|}{n}$\n",
    "\n",
    "Another common error function is the Root Mean Squared Error (RMSE), which only difference from the MAE is to take the squared root of the sum's total (instead of dividing it by the number of input values):\n",
    "\n",
    "$\\large \\sqrt{81 + 16 + 25 + 49 + 49} = 6.63$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Median\n",
    "---\n",
    "\n",
    "## Symbol\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\tilde{X}\n",
    "$\n",
    "or\n",
    "$\n",
    "\\Large\n",
    "M\n",
    "$\n",
    "or\n",
    "$\n",
    "\\Large\n",
    "Med\n",
    "$\n",
    "\n",
    "Formally, a median of a population is any value such that at most half of the population is less than the proposed median and at most half is greater than the proposed median.\n",
    "\n",
    "Medians may not be unique. If each set contains less than half the population, then some of the population is exactly equal to the unique median.\n",
    "\n",
    "The median is well-defined for any ordered (one-dimensional) data, and is independent of any distance metric. The median can thus be applied to classes which are ranked but not numerical (e.g. working out a median grade when students are graded from A to F), although the result might be halfway between classes if there is an even number of cases.\n",
    "\n",
    "The median can be used as a measure of location when one attaches reduced importance to extreme values, typically because a distribution is skewed, extreme values are not known, or outliers are untrustworthy, i.e., may be measurement/transcription errors.\n",
    "\n",
    "## Examples\n",
    "\n",
    "$\n",
    "\\large\n",
    "\\tilde{X} = \\Large\\frac{1}{2} (x_{\\lfloor (n+1)/2\\rfloor} + x_{\\lceil (n+1)/2\\rceil})\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum Algorithm\n",
    "---\n",
    "\n",
    "The gradient descent with momentum algorithm (or Momentum for short) borrows the idea from physics. Imagine rolling down a ball inside of a frictionless bowl. Instead of stopping at the bottom, the momentum it has accumulated pushes it forward, and the ball keeps rolling back and forth.\n",
    "\n",
    "![Momentum Algorithm](images\\momentum.gif \"Momentum Algorithm\")\n",
    "\n",
    "We can apply the concept of momentum to our vanilla gradient descent algorithm. In each step, in addition to the regular gradient, it also adds on the movement from the previous step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Logarithm\n",
    "---\n",
    "\n",
    "## Symbol\n",
    "\n",
    "$\\LARGE \\ln x$\n",
    "or\n",
    "$\\LARGE \\ln(x)$\n",
    "or\n",
    "$\\LARGE \\log x$\n",
    "or\n",
    "$\\LARGE \\log(x)$\n",
    "or\n",
    "$\\LARGE \\log_{e}x$\n",
    "or\n",
    "$\\LARGE \\log_{e}(x)$\n",
    "\n",
    "Logarithms are useful for solving equations in which the unknown appears as the exponent of some other quantity. For example, logarithms are used to solve for the half-life, decay constant, or unknown time in exponential decay problems. They are important in many branches of mathematics and scientific disciplines, and are used in finance to solve problems involving compound interest.\n",
    "\n",
    "The natural logarithm of a number is its logarithm to the base of the mathematical constant $e$. The natural logarithm of $x$ is generally written as $\\ln x$, $\\log_{e} x$, or sometimes, if the base $e$ is implicit, simply $\\log x$. To prevent ambiguity, parentheses may also be added, giving $\\ln(x)$, $\\log_{e}(x)$, or $\\log(x)$, particularly when the argument to the logarithm is not a single symbol.\n",
    "\n",
    "The natural logarithm of $x$ is the power to which $e$ would have to be raised to equal $x$. The natural logarithm of $e$ itself, $\\ln_{e}$, is $1$, because $e^{1} = e$, while the natural logarithm of $1$ is $0$, since $e^{0} = 1$.\n",
    "\n",
    "The natural logarithm can be defined for any positive real number $a$ as the area under the curve $y = 1/x$ from $1$ to $a$ (with the area being negative when $0 < a < 1$):\n",
    "\n",
    "![Graph of the natural logarithm function](images\\natural_logarithm.png \"Graph of the natural logarithm function\")\n",
    "\n",
    "The simplicity of this definition, which is matched in many other formulas involving the natural logarithm, leads to the term \"natural\".\n",
    "\n",
    "## Examples\n",
    "\n",
    "The natural logarithm of $x$ is the power to which $e$ would have to be raised to equal $x$:\n",
    "\n",
    "$\\ln{(7.5)} = 2.0149030205422647$ because $\\large e^{2.0149030205422647} \\normalsize = 7.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "round(math.pow(math.e, math.log(7.5, math.e)), 14) == 7.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Numbers\n",
    "---\n",
    "\n",
    "## Symbol\n",
    "\n",
    "$\n",
    "\\LARGE\n",
    "\\mathbb{N}\n",
    "$\n",
    "\n",
    "In mathematics, the natural numbers are those used for counting (as in \"there are six coins on the table\") and ordering (as in \"this is the third largest city in the country\"). In common mathematical terminology, words colloquially used for counting are \"cardinal numbers\", and words used for ordering are \"ordinal numbers\". The natural numbers can, at times, appear as a convenient set of codes (labels or \"names\"); that is, as what linguists call nominal numbers, forgoing many or all of the properties of being a number in a mathematical sense. The set of natural numbers is often denoted by the symbol $\\mathbb{N}$.\n",
    "\n",
    "Some definitions, including the standard ISO 80000-2, begin the natural numbers with $0$, corresponding to the non-negative integers $0$, $1$, $2$, $3$, ... (collectively denoted by the symbol $\\mathbb{N}_{0}$), whereas others start with $1$, corresponding to the positive integers $1$, $2$, $3$, ... (collectively denoted by the symbol $\\mathbb{N}_{1}$).\n",
    "\n",
    "Texts that exclude zero from the natural numbers sometimes refer to the natural numbers together with zero as the whole numbers, while in other writings, that term is used instead for the integers (including negative integers).\n",
    "\n",
    "Properties of the natural numbers, such as divisibility and the distribution of prime numbers, are studied in number theory. Problems concerning counting and ordering, such as partitioning and enumerations, are studied in combinatorics.\n",
    "\n",
    "In common language, particularly in primary school education, natural numbers may be called counting numbers to intuitively exclude the negative integers and zero, and also to contrast the discreteness of counting to the continuity of measurement — a hallmark characteristic of real numbers.\n",
    "\n",
    "## Examples\n",
    "\n",
    "* $\\large \\mathbb{N}_{0} = \\mathbb{N}^{0} = \\mathbb{N} \\cup \\{0\\} = \\{0, 1, 2, 3, \\dots\\}$\n",
    "* $\\large \\mathbb{N}^{*} = \\mathbb{N}^{+} = \\mathbb{N}_{1} = \\mathbb{N}_{> 0} = \\{1, 2, 3, \\dots\\}$"
   ]
  },
  {
   "source": [
    "# Neural Network\n",
    "---\n",
    "![Neural Network Representation](images\\neural_network_representation.png \"Neural Network Representation\")\n",
    "\n",
    "## Notation\n",
    "\n",
    " * Vector x = Input features. An alternative notation for the input features is $a^{[0]}$ (because $x^{[0]} = a^{[0]}$).\n",
    " * By convention, the input layer does not count for the number of layers in the network. The first layer that gets counted is the first layer (an hidden layer). However, even in a $2$-layer neural network there will be also an input layer (so, the $2$-layer network will actually have $3$ layers of neurons).\n",
    " * The input layer is then designated as layer number zero. The term $a$ also stands for activations. It refers to the values that the neurons $n$ the network are passing on to the subsquent layers, e.g. $a^{[0]} = X$.\n",
    " * Superscript square brackets = refer to a specific layer of neurons, e.g. $z^{[2]}$.\n",
    " * Subscript number - refer to a specific node in a specific layer, e.g. this refers to the first node in layer $2$: $z_{1}^{[2]}$.\n",
    " * Based on the previous, this is the notation for parameter $a$, referring to training example $i$, in layer $2$: $a^{[2](i)}$. \n",
    " * $w$ and $b$ are parameters associated with layers one and following, e.g. in a $2$-layer neural network, there will be $w^{[1]}$ and $b^{[1]}$ parameters associated with layer one, and then $w^{[2]}$ associated with layer two (the output layer). Assuming there are $3$ features ($x1$, $x2$ and $x3$), layer one will have $4$ neurons, $w^{[1]}$ will have a size of $(4, 3)$ abd $b^{[1]}$ will have a size of $(4,1)$. Then, in layer two (the output layer), $w^{[2]}$ will have a size of $(1, 4)$ and $b^{[2]}$ will have a size of $(1, 1)$.\n",
    "\n",
    "## Example\n",
    "\n",
    "Consider the neural network architecture below. Its parameters have the following shapes: $z^{[1]} = (4, 1)$, $W^{[1]} = (4, 3)$, $x = (3, 1)$, $b^{[1]} = (4, 1)$, and $a^{[1]} = (4, 1)$. Note that $a^{[0]}$ is just an alias for $x$, so its shape is the same of $x$.  \n",
    "\n",
    "![Neural Network Model](images\\neural_network_model_2.png \"Neural Network Model\")\n",
    "\n",
    "When compared to for-loops, a vectorized implementation provides the same functionality, however, it is computationally much more efficient. Concretely, through the matrix operations it is possible to compute all the parameters we need on any network layer using a few instructions.\n",
    "\n",
    "When implementing vectorization, horizontally, the columns of $X$ contain the training examples, and vertically, $X$ rows contain the input features. Regarding $Z$ and $A$, horizontally, their columns contain the training examples, while vertically, their rows refer to the hidden units (the neurons on the hidden layers).\n",
    "\n",
    "### Parameters Initialization\n",
    "\n",
    "To ensure breaking of parameter symmetry, $W$ must be with initialized with a random set of small values, so that its parameters can learn independently from each other:\n",
    "$\\\\ \\large W^{[1]} = \\text{np.random.randn}((2, 2)) \\cdot 0.01$\n",
    "\n",
    "The symmetry problem does not affect $b$, so we can initialize it with zeros:\n",
    "$\\\\ \\large b^{[1]} = \\text{np.zeros}((2, 1))$\n",
    "\n",
    "$\\large W^{[2]} = \\text{np.random.randn}((1, 2)) \\cdot 0.01$\n",
    "\n",
    "$\\large b^{[2]} = \\text{np.zeros}((1, 1))$\n",
    "\n",
    "Note: Logistic Regression doesn't have a hidden layer, so if we initialize the weights to zeros, the first example $x$ fed in the logistic regression will output zero but the derivatives of the Logistic Regression depend on the input $x$ (because there's no hidden layer) which is not zero. So, at the second iteration, the weights values follow $x$'s distribution and will be different from each other if $x$ is not a constant vector. \n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "$\\large Z^{[1]} = W^{[1]} X + b^{[1]}$\n",
    "\n",
    "$\\large A^{[1]} = g(Z^{[1]})$\n",
    "\n",
    "$\\large Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$\n",
    "\n",
    "$\\large A^{[2]} = g(Z^{[2]})$\n",
    "\n",
    "After the last step (the previous expression), we compute the loss:\n",
    "$\\\\ \\large L(a^{[2]}, y)$\n",
    "\n",
    "### Back Propagation\n",
    "\n",
    "This expression enable skipping the computation of the $a$ derivative from the last step in forward propagation:\n",
    "$\\\\ \\large dZ^{[2]} = A^{[2]} - Y$\n",
    "\n",
    "$\\large dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T}$\n",
    "\n",
    "This sum can be coded as np.sum(dZ, axis=1, keepdims=True):\n",
    "$\\\\ \\large db^{[2]} = \\frac{1}{m} \\sum dZ^{[2]}$\n",
    "\n",
    "This multiplication is element-wise:\n",
    "$\\\\ \\large dZ^{[1]} = W^{[2]T} dZ^{[2]} \\cdot g^{[1]}\\prime(Z^{[1]})$\n",
    "\n",
    "$\\large dW^{[1]} = \\frac{1}{m} dZ^{[1]} X^{T}$\n",
    "\n",
    "This sum can be coded as np.sum(dZ, axis=1, keepdims=True):\n",
    "$\\\\ \\large db^{[1]} = \\frac{1}{m} \\sum dZ^{[1]}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nominal And Ordinal Data\n",
    "---\n",
    "\n",
    "Nominal data are those items which are distinguished by a simple naming system. They are data with no numeric value, such as profession. The nominal data just name a thing without applying it to an order related to other numbered items.\n",
    "\n",
    "The most popular way of thinking about nominal data and variables is that they are just named.\n",
    "\n",
    "Nominal data are also called categorical data. In the nominal scale, the subjects are only allocated to different categories. The values grouped into these categories have no meaningful order. There is no hierarchy. For example, gender and occupation are nominal level values.\n",
    "\n",
    "Ordinal data is data which is placed into some kind of order by their position on the scale. For example, they may indicate superiority. However, you cannot do arithmetic with ordinal numbers because they only show sequence.\n",
    "\n",
    "Ordinal data and variables are considered as “in between” categorical and quantitative variables. In other words, the ordinal data is categorical data for which the values are ordered.\n",
    "\n",
    "![Nominal vs Ordinal Data](images\\nominal_vs_ordinal_data.png \"Nominal vs Ordinal Data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Norm\n",
    "---\n",
    "\n",
    "In mathematics, a norm is a function from a vector space over the real or complex numbers to the non-negative real numbers, that satisfies certain properties pertaining to scalability and additivity and takes the value zero only if the input vector is zero. A pseudonorm or seminorm satisfies the same properties, except that it may have a zero value for some nonzero vectors.\n",
    "\n",
    "The Euclidean norm, or 2-norm, is a specific norm on a Euclidean vector space that is strongly related to the Euclidean distance. It is also equal to the square root of the inner product of a vector with itself.\n",
    "\n",
    "A vector space on which a norm is defined is called a normed vector space. In a similar manner, a vector space with a seminorm is called a seminormed vector space.\n",
    "\n",
    "Some of the most useful operators in linear algebra are norms. Informally, the norm of a vector tells us how big a vector is. The notion of size under consideration here concerns not dimensionality but rather the magnitude of the components. A vector norm is a function $f$ that maps a vector to a scalar, satisfying a handful of properties.\n",
    "\n",
    "In deep learning, we are often trying to solve optimization problems: _maximize_ the probability assigned to observed data; _minimize_ the distance between predictions and the ground-truth observations. Assign vector representations to items (like words, products, or news articles) such that the distance between similar items is minimized, and the distance between dissimilar items is maximized. Oftentimes, the objectives, perhaps the most important components of deep learning algorithms (besides the data), are expressed as norms.\n",
    "\n",
    "## Examples\n",
    "\n",
    "The Euclidean distance is a norm: specifically, it is the $L2$ norm. Suppose that the elements in the $n$-dimensional vector $x$ are $x_{1}, \\, \\ldots \\,, \\, x_{n}$. The $L_{2}$ norm of $x$ is the square root of the sum of the squares of the vector elements:\n",
    "\n",
    "$\\large \\|\\textbf{x}\\|_{2} = \\sqrt{\\sum\\limits_{i=1}^{n} x_{i}^{2}}$\n",
    "\n",
    "In deep learning, the squared $L_{2}$ norm is more often used, however, it is also frequent to find the $L_{1}$ norm, which is expressed as the sum of the absolute values of the vector elements:\n",
    "\n",
    "$\\large \\|\\textbf{x}\\|_{1} = \\sum\\limits_{i=1}^{n} |x_{i}|$\n",
    "\n",
    "![Taxicab geometry versus Euclidean distance](images\\taxicab_geometry.png \"Taxicab geometry versus Euclidean distance\")\n",
    "\n",
    "In _taxicab geometry_ (a $L_{1}$ norm example), the red, yellow, and blue paths all have the same shortest path length of $12$. In Euclidean geometry, the green line has length $6 \\sqrt{2} \\approx 8.49$ and is the unique shortest path.\n",
    "\n",
    "As compared with the $L_{2}$ norm, the $L_{1}$ norm is less influenced by outliers.\n",
    "\n",
    "Both the $L_{2}$ norm and the $L_{1}$ norm are special cases of the more general $L_{p}$ norm:\n",
    "\n",
    "$\\large \\|\\textbf{x}\\|_{p} = \\left( \\sum\\limits_{i=1}^{n} |x_{i}|^{p} \\right)^{1/p}$\n",
    "\n",
    "Analogous to $L_{2}$ norms of vectors, the Frobenius norm of a matrix $X \\in \\mathbb{R}^{m \\times n}$ is the square root of the sum of the squares of the matrix elements:\n",
    "\n",
    "$\\large \\|\\textbf{X}\\|_{F} = \\sqrt{\\sum\\limits_{i=1}^{m} \\sum\\limits_{j=1}^{n} x_{ij}^{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7\n5.0\n6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "u = np.array([3, -4])\n",
    "\n",
    "# Calculate the L1 norm of a vector\n",
    "# Output is 7\n",
    "print(np.abs(u).sum())\n",
    "\n",
    "# Calculate the L2 norm of a vector\n",
    "# Output is 5.0\n",
    "print(np.linalg.norm(u))\n",
    "\n",
    "# Calculate the Frobenius norm of a matrix\n",
    "# Output is 6.0\n",
    "print(np.linalg.norm(np.ones((4, 9))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Distribution\n",
    "---\n",
    "\n",
    "## Symbol\n",
    "\n",
    "$\n",
    "\\LARGE\n",
    "\\mathcal{N}(\\mu, \\sigma^2)\n",
    "$\n",
    "\n",
    "In probability theory, a normal distribution (or Gaussian or Gauss or Laplace–Gauss or \"bell curve\" - though there are other bell shaped curves which are not a normal distribution) is a type of continuous probability distribution for a real-valued random variable. The general form of its PDF (Probability Density Function) is:\n",
    "\n",
    "$\\Large f(x) = \\frac{1}{\\sigma\\sqrt{2 \\pi}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu}{\\sigma} \\right)^2}$\n",
    "\n",
    "The parameter $\\mu$ is the mean or expectation of the distribution (and also its median and mode), while the parameter $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$. A random variable with a Gaussian distribution is said to be normally distributed, and is called a normal deviate.\n",
    "\n",
    "Normal distributions are important in statistics and are often used in the natural and social sciences to represent real-valued random variables whose distributions are not known. Their importance is partly due to the central limit theorem. It states that, under some conditions, the average of many samples (observations) of a random variable with finite mean and variance is itself a random variable whose distribution converges to a normal distribution as the number of samples increases. Therefore, physical quantities that are expected to be the sum of many independent processes, such as measurement errors, often have distributions that are nearly normal.\n",
    "\n",
    "## Examples\n",
    "\n",
    "### Standard normal distribution\n",
    "\n",
    "The simplest case of a normal distribution is known as the standard normal distribution. This is a special case when $\\mu = 0$ and $\\sigma = 1$, and it is described by this probability density function (PDF):\n",
    "\n",
    "$\\large \\varphi(x) = \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{1}{2}x^{2}}$\n",
    "\n",
    "### General normal distribution\n",
    "\n",
    "Every normal distribution is a version of the standard normal distribution, whose domain has been stretched by a factor $\\sigma$ (the standard deviation) and then translated by $\\mu$ (the mean value):\n",
    "\n",
    "$\\large f(x \\, | \\, \\mu, \\sigma^2) = \\frac{1}{\\sigma} \\varphi \\left( \\frac{x - \\mu}{\\sigma} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outcome\n",
    "---\n",
    "\n",
    "In probability theory, an outcome is a possible result of an experiment or trial. Each possible outcome of a particular experiment is unique, and different outcomes are mutually exclusive (only one outcome will occur on each trial of the experiment).\n",
    "\n",
    "The set of all the possible outcomes of an experiment form the elements of a _sample space_, which is usually noted as $S$ or $\\Omega$.\n",
    "\n",
    "For the experiment where we flip a coin twice, the four possible outcomes that make up our sample space are $(H, T), (T, H), (T, T)$ and $(H, H)$, where \"$H$\" represents a \"heads\", and \"$T$\" represents a \"tails\". Outcomes should not be confused with events, which are sets (or informally, \"groups\") of outcomes. For comparison, we could define an event to occur when \"at least one _heads_\" is flipped in the experiment - that is, when the outcome contains at least one _heads_. This event would contain all outcomes in the sample space except the element $(T, T)$.\n",
    "\n",
    "Since individual outcomes may be of little practical interest, or because there may be prohibitively (even infinitely) many of them, outcomes are grouped into sets of outcomes that satisfy some condition, which are called \"events.\" The collection of all such events is a sigma-algebra.\n",
    "\n",
    "An event containing exactly one outcome is called an _elementary event_. The event that contains all possible outcomes of an experiment is its sample space. A single outcome can be a part of many different events.\n",
    "\n",
    "## Examples\n",
    "\n",
    " 1. An example of a simple random experiment (e.g. $E_{1}$) is to toss a die and observe the outcome. In this case, the sample space of $E_{1}$ is: $S_{1} = \\{1, 2, 3, 4, 5, 6\\}$, and $S'_{1} = \\{$\"odd\"$, \\, $\"even\"$\\}$. This also shows that the sample space for a given experiment may not be unique.\n",
    "\n",
    " 2. The sample space for tossing a coin is (assuming \"$H$ = Heads\" and \"$T$ = Tails\"): \n",
    " $S_{2} = \\{ HHH, HHT, HTH, HTT, THH, THT, TTH, TTT\\}$\n",
    "\n",
    " 3. See how long a bulb lasts: $S_{3} = \\{ \\, t \\, | t \\geq 0\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation\n",
    "---\n",
    "\n",
    "In mathematics, a permutation of a set is, loosely speaking, an arrangement of its members into a sequence or linear order, or if the set is already ordered, a rearrangement of its elements. The word \"permutation\" also refers to the act or process of changing the linear order of an ordered set.\n",
    "\n",
    "Permutations differ from combinations, which are selections of some members of a set regardless of order. For example, written as tuples, there are six permutations of the set $\\{1,2,3\\}$, namely: $(1,2,3)$, $(1,3,2)$, $(2,1,3)$, $(2,3,1)$, $(3,1,2)$, and $(3,2,1)$. These are all the possible orderings of this three-element set. Anagrams of words whose letters are different are also permutations: the letters are already ordered in the original word, and the anagram is a reordering of the letters. The study of permutations of finite sets is an important topic in the fields of combinatorics and group theory.\n",
    "\n",
    "Permutations are used in almost every branch of mathematics, and in many other fields of science. In computer science, they are used for analyzing sorting algorithms; in quantum physics, for describing states of particles; and in biology, for describing RNA sequences.\n",
    "\n",
    "The number of permutations of $n$ distinct objects is $n$ factorial, usually written as $n!$, which means the product of all positive integers less than or equal to $n$.\n",
    "\n",
    "## Examples\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pi\n",
    "---\n",
    "\n",
    "## Symbol\n",
    "\n",
    "$\n",
    "\\Huge\n",
    "\\pi\n",
    "$\n",
    "\n",
    "The number $\\pi$ is a mathematical constant. $\\pi$ is commonly defined as the ratio of a circle's circumference $C$ to its diameter $d$:\n",
    "\n",
    "$\\Large \\pi = \\frac{C}{d}$\n",
    "\n",
    "![Pi represents a ratio between a circumference perimeter and its diameter](images\\pi.png \"Representation of pi as a ratio between a circumference perimeter and its diameter\")\n",
    "\n",
    "$\\large\\pi$ appears in many formulas in all areas of mathematics and physics. Its value is approximately equal to $3.141592653589793$. It has been represented by the Greek letter $\\pi$ since the mid-18th century, and it is also referred to as \"Archimedes' constant\".\n",
    "\n",
    "Being an __irrational number__, $\\pi$ cannot be expressed as a common fraction. Although fractions such as $22/7$ and $355/113$ are commonly used as an approximation, no common fraction (ratio of whole numbers) can be its exact value.\n",
    "\n",
    "Equivalently, $\\pi$ decimal representation never ends and never settles into a permanently repeating pattern. Its decimal (or other base) digits appear to be randomly distributed, and are conjectured to satisfy a specific kind of statistical randomness.\n",
    "\n",
    "It is known that $\\pi$ is also a _transcendental number_, meaning, it is not the root of any polynomial with rational coefficients. The transcendence of $\\pi$ implies that it is impossible to solve the ancient challenge of squaring the circle with a compass and straightedge.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Examples\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Slope Form\n",
    "---\n",
    "\n",
    "## Formula\n",
    "\n",
    "$\n",
    "\\Large\n",
    "y - y_{1} = m(x - x_{1})\n",
    "$\n",
    "\n",
    "The point slope form is an equation of a straight line. In this formula, $y$ is the unknown $y$ coordinate, and $y_{1}$ is the given $y$ coordinate, which is to be placed in the formula.\n",
    "\n",
    "Similarly, $x$ is the unknown $x$ coordinate, and $x_{1}$ is the given $x$ coordinate, which is to be used in the formula.\n",
    "\n",
    "The variable $m$ represents the slope of the straight line.\n",
    "\n",
    "We can use the point slope formula when we have the coordinates of one point on the line and the slope of the line.\n",
    "\n",
    "## Examples\n",
    "\n",
    "Find the equation of the straight line that has slope $m = 4$ and passes through the point $(–1, –6)$:\n",
    "\n",
    "$\n",
    "y - y_{1} = m(x - x_{1})\\\\\n",
    "y - (-6) = (4)(x - (-1))\\\\\n",
    "y + 6 = 4(x + 1)\\\\\n",
    "y + 6 = 4x + 4\\\\\n",
    "y = 4x + 4 - 6\\\\\n",
    "y = 4x - 2\n",
    "$"
   ]
  },
  {
   "source": [
    "# Precision, Recall And F1-Score\n",
    "---\n",
    "## Formulae\n",
    "\n",
    "### Precision or Positive Predictive Value (PPV)\n",
    "\n",
    "$\\Large PPV = \\frac{TP}{TP + FP} = 1 - FDR$\n",
    "\n",
    "### Specificity, Selectivity or True Negative Rate (TNR)\n",
    "\n",
    "$\\Large TNR = \\frac{TN}{N} = \\frac{TN}{TN + FP} = 1 - FPR$\n",
    "\n",
    "### Recall, Sensitivity, Hit Rate or True Positive Rate (TPR)\n",
    "\n",
    "$\\Large TPR = \\frac{TP}{P} = \\frac{TP}{TP + FN} = 1 - FNR$\n",
    "\n",
    "### Miss Rate or False Negative Rate (FNR)\n",
    "\n",
    "$\\Large FNR = \\frac{FN}{P} = \\frac{FN}{FN + TP} = 1 - TPR$\n",
    "\n",
    "### Fall-Out or False Positive Rate (FPR)\n",
    "\n",
    "$\\Large FPR = \\frac{FP}{N} = \\frac{FP}{FP + TN} = 1 - TNR$\n",
    "\n",
    "### Accuracy (ACC)\n",
    "\n",
    "$\\Large ACC = \\frac{TP + TN}{P + N} = \\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "### F1-Score\n",
    "\n",
    "$\\Large F_{1} \\, Score = 2 \\frac{PR}{P + R}$\n",
    "\n",
    "In pattern recognition, information retrieval and classification (machine learning), precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of retrieved relevant instances among all relevant instances. Both precision and recall are therefore based on an understanding and measure of relevance.\n",
    "\n",
    "When measuring a set, accuracy is closeness of the measurements to a specific value, while precision is the closeness of the measurements to each other. The field of statistics, where the interpretation of measurements plays a central role, prefers to use the terms bias and variability instead of accuracy and precision: bias is the amount of inaccuracy and variability is the amount of imprecision.\n",
    "\n",
    "![Precision and Recall](images\\precision_and_recall.png \"Precision and Recall\")\n",
    "\n",
    "In statistical analysis of binary classification, the F-score or F-measure is a measure of a test's accuracy. It is calculated from the precision and recall of the test, where the precision is the number of correctly identified positive results divided by the number of all positive results, including those not identified correctly, and the recall is the number of correctly identified positive results divided by the number of all samples that should have been identified as positive.\n",
    "\n",
    "The $F_{1}$-score is the harmonic mean of precision and recall. The more generic $F_{\\beta}$-score applies additional weights, valuing one of precision or recall more than the other.\n",
    "\n",
    "### Example\n",
    "\n",
    "An example illustrating why $F_{1}$-score is a better measure of accuracy when compared to a simple average:\n",
    "\n",
    "![Precision and Recall Example](images\\precision_and_recall_example.png \"Precision and Recall Example\")"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prime Number\n",
    "---\n",
    "\n",
    "A prime number (or a prime) is a natural number greater than $1$ that is not a product of two smaller natural numbers. A natural number greater than $1$ that is not prime is called a composite number. For example, $5$ is prime because the only ways of writing it as a product, $1 \\, \\cdot \\, 5$ or $5 \\, \\cdot \\, 1$, involve $5$ itself. However, $4$ is composite because it is a product $(2 \\, \\cdot \\, 2)$ in which both numbers are smaller than $4$.\n",
    "\n",
    "Primes are central in number theory because of the fundamental theorem of arithmetic: every natural number greater than $1$ is either a prime itself or can be factorized as a product of primes that is unique up to their order.\n",
    "\n",
    "Composite numbers can be arranged into rectangles, but prime numbers cannot:\n",
    "\n",
    "![Prime vs composite numbers](images\\prime_numbers.png \"Prime vs composite numbers\")\n",
    "\n",
    "There are infinitely many primes, as demonstrated by Euclid around $300$ BC. No known simple formula separates prime numbers from composite numbers. However, the distribution of primes within the natural numbers in the large can be statistically modelled. The first result in that direction is the prime number theorem, proven at the end of the $19$th century, which says that the probability of a randomly chosen number being prime is inversely proportional to its number of digits, that is, to its logarithm.\n",
    "\n",
    "## Examples\n",
    "\n",
    "The first $25$ prime numbers (which are all the prime numbers less than $100$):\n",
    "\n",
    "$2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89$ and $97$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability\n",
    "---\n",
    "\n",
    "## Symbol\n",
    "\n",
    "$\\Large P(A)$\n",
    "or\n",
    "$\\Large p(A)$\n",
    "or\n",
    "$\\Large Pr(A)$\n",
    "\n",
    "## General Multiplication Rule\n",
    "\n",
    "The general multiplication rule states that the probability of any two events, $A$ and $B$, both happening can be calculated as:\n",
    "\n",
    "$\\Large P(A \\, \\vert \\, B) = \\frac{P(A \\, \\cap \\, B)}{P(B)}$\n",
    "\n",
    "The vertical bar $\\vert$ means \"given\". Thus, $P(B \\vert A)$ can be read as \"the probability that $B$ occurs, given that $A$ has occurred\".\n",
    "\n",
    "If events $A$ and $B$ are independent, then $P(B \\vert A)$ is simply equal to $P(B)$ and the rule can be simplified to:\n",
    "\n",
    "$\\Large P(A \\, \\text{and} \\, B) = P(A) \\cdot P(B)$\n",
    "\n",
    "Probability is the branch of mathematics concerning numerical descriptions of how likely an event is to occur, or how likely it is that a proposition is true. Probability describes random variation in systems. The probability of an event is a number between $0$ and $1$, where, roughly speaking, $0$ indicates impossibility of the event and $1$ indicates certainty.\n",
    "\n",
    "The higher the probability of an event, the more likely it is that the event will occur. A simple example is the tossing of a fair (unbiased) coin. Since the coin is fair, the two outcomes (\"heads\" and \"tails\") are both equally probable; the probability of \"heads\" equals the probability of \"tails\"; and since no other outcomes are possible, the probability of either \"heads\" or \"tails\" is $1/2$ (which could also be written as $0.5$ or $50\\%$).\n",
    "\n",
    "A joint probability is the probability that two separate events with separate probability distributions are both true. $P(A$ and $B)$ is written $P(A, B)$, and read “the joint probability of $A$ and $B$” or “the probability that $A$ is true and $B$ is true.”\n",
    "\n",
    "The _opposite_ or _complement_ of an event $A$ is the event $\\sim A$ (that is, the event of $A$ not occurring), often denoted as $A', \\, A^{c}, \\, \\overline{A}, \\, A^\\complement, \\, \\neg A$, or $\\sim A$. Its probability is given by $P(\\sim A) = 1 − P(A)$.\n",
    "\n",
    "Probability theory is applied in everyday life in risk assessment and modeling. The insurance industry and markets use actuarial science to determine pricing and make trading decisions. Governments apply probabilistic methods in environmental regulation, entitlement analysis (reliability theory of aging and longevity), and financial regulation.\n",
    "\n",
    "### Axioms\n",
    "\n",
    " 1. $0 \\leq P(A) \\leq 1$: a probability value is always between $0$ and $1$.\n",
    " 2. $P(S) = 1$: the probability of having an outcome is $1$.\n",
    " 3. If $A \\cap B = \\emptyset$ then $P(A \\cup B) = P(A) + P(B)$.\n",
    "\n",
    "### Theorems\n",
    "\n",
    " 1. $P(\\emptyset) = 0$.\n",
    " 2. $P(A' = 1 - P(A)$.\n",
    " 3. $P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$.\n",
    " 4. For any events $A, B, C$: $P(A \\cup B \\cup C) = P(A) + P(B) + P(C) - P(A \\cap B) - P(A \\cap C) - P(B \\cap C) + P(A \\cap B \\cap C)$.\n",
    " 5. $A \\leq B = P(A) \\leq P(B)$.\n",
    "\n",
    "## Examples\n",
    "\n",
    " 1. Find the probability that the event of heads comes up after tossing a coin. This sample space is $S = \\{H, T\\}$, therefore, $P(H) = \\frac{1}{2} = 50$% probability.\n",
    "\n",
    " 2. Find the probability of raining tomorrow, assuming:\n",
    "\n",
    "    - $40$% chance of cold ($C$).\n",
    " \n",
    "    - $10$% chance of cold and rain ($C$ and $R$).\n",
    " \n",
    "    - $80$% chance of cold or rain ($C$ or $R$, or both)\n",
    "  \n",
    "    So, $P(R) = P(C \\cup R) - P(C) + P(C \\cap R).\n",
    "    \n",
    "    Then, 0.8 - 0.4 + 0.1 = 0.5 = 50$% probability.\n",
    "\n",
    " 3. Find the probability that any person likes at least one of the three following activities, assuming:\n",
    "\n",
    "    - $75$% people like jogging ($J$).\n",
    "\n",
    "    - $20$% like ice cream ($I$).\n",
    "\n",
    "    - $40$% enjoy music ($M$).\n",
    "\n",
    "    - $15$% $J \\cap I$.\n",
    "\n",
    "    - $30$% $J \\cap M$.\n",
    "\n",
    "    - $10$% $I \\cap M$.\n",
    "\n",
    "    - $5$% $J \\cap I \\cap M$.\n",
    "\n",
    "   So, $P(J \\cup I \\cup M) = P(J) + P(I) + P(M) - P(J \\cap I) - P(J \\cap M) - P(I \\cap M) + P(J \\cap I \\cap M)$.\n",
    "   \n",
    "   Then, $0.75 + 0.20 + 0.40 - 0.15 - 0.30 - 0.10 + 0.05 = 0.85 = 85$% probability.\n",
    "\n",
    " 4. Based on previous example assumptions, find the probability of one person to like only one of the three activities (e.g. to like jogging, but not ice cream or music). \n",
    "\n",
    " So, $P(J \\cap \\overline{I} \\cap \\overline{M}) + P(\\overline{J} \\cap I \\cap \\overline{M}) + P(\\overline{J} \\cap \\overline{I} \\cap M) = 0.35 + 0.0 + 0.05 = 0.40$.\n",
    " \n",
    " This result is better illustrated using a Venn diagram:\n",
    "\n",
    " ![Venn diagram for probability example 4](images\\probability_venn_diagram_01.png \"Venn diagram for probability example 4\")\n",
    "\n",
    " Note: The \"principle of inclusion-exclusion\" generalizes this case to $n$ events, e.g. $A, B, C, D, E$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Density Function\n",
    "---\n",
    "\n",
    "In probability theory, a probability density function (PDF), or density of a continuous random variable, is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the random variable would equal that sample.\n",
    "\n",
    "In other words, while the absolute likelihood for a continuous random variable to take on any particular value is $0$ (since there are an infinite set of possible values to begin with), the value of the PDF at two different samples can be used to infer, in any particular draw of the random variable, how much more likely it is that the random variable would equal one sample compared to the other sample.\n",
    "\n",
    "In a more precise sense, the PDF is used to specify the probability of the random variable falling within a particular range of values, as opposed to taking on any one value. This probability is given by the integral of this variable's PDF over that range—that is, it is given by the area under the density function but above the horizontal axis and between the lowest and greatest values of the range. The probability density function is non-negative everywhere, and its integral over the entire space is equal to $1$.\n",
    "\n",
    "![Probability density function of a normal distribution](images\\probability_density_function.png \"Probability density function of a normal distribution graph\")\n",
    "\n",
    "## Examples\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Distribution\n",
    "---\n",
    "\n",
    "In probability theory and statistics, a probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment. It is a mathematical description of a random phenomenon in terms of its sample space and the probabilities of events (subsets of the sample space).\n",
    "\n",
    "What defines a probability distribution is a collection of statements, two or more, where those statements are exclusive and exahustive. Exclusive means that no more than one statement can be true. Exhaustive means that, assuming we have complete information, at least one statement must be true. \n",
    "\n",
    "For instance, if $X$ is used to denote the outcome of a coin toss (\"the experiment\"), then the probability distribution of $X$ would take the value $0.5$ for $X$=heads, and $0.5$ for $X$=tails (assuming that the coin is fair).\n",
    "\n",
    "Examples of random phenomena include the weather condition in a future date, the height of a person, the fraction of male students in a school, the results of a survey, etc.\n",
    "\n",
    "A probability distribution can be described in various forms, such as by a probability mass function or a cumulative distribution function. One of the most general descriptions, which applies for continuous and discrete variables, is by means of a probability function $P:\\mathcal{A} \\rightarrow \\mathbb{R}$ whose input space $\\mathcal{A}$ is related to the sample space, and gives a probability as its output.\n",
    "\n",
    "## Examples\n",
    "\n",
    "### Complement Rule\n",
    "\n",
    "$\\large P(A) = 1 - P(A^{c})$\n",
    "\n",
    "### Addition Rule for Mutually Exclusive Events\n",
    "\n",
    "$\\large P(A$ or $B) = P(A) + P(B)$\n",
    "\n",
    "### Multiplication Rule for Independent Events\n",
    "\n",
    "We can calculate the probability of two events from independent distributions occuring at the same time. The probability independence definition states that when the joint distribution equals the product distribution, the two distributions are independent.\n",
    "\n",
    "So, for example, considering the probability of getting heads from throwing a fair coin to be $1/2$, and the probability of getting a $3$ when throwing a fair dice to be $1/3$, and as those two events belong to independent distributions, we can calculate their joint probability using the following formula:\n",
    "\n",
    "$\\large P(x_{1}, y_{1}) = P(x_{1}) \\cdot P(y_{1}) = \\left(\\frac{1}{2}\\right) \\cdot \\left(\\frac{1}{6}\\right) = \\frac{1}{12}$\n",
    "\n",
    "### _At Least One_ Rule\n",
    "\n",
    "$\\large P($at least one$) \\,\\, = 1 - P($none$)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Space\n",
    "---\n",
    "\n",
    "In probability theory, a probability space or a probability triple $(\\Omega ,{\\mathcal {F}},P)$ is a mathematical construct that provides a formal model of a random process or \"experiment\". For example, one can define a probability space which models the throwing of a die.\n",
    "\n",
    "A probability space consists of three elements:\n",
    "\n",
    "* A sample space, $\\Omega$, which is the set of all possible outcomes.\n",
    "* An event space, which is a set of events, $\\mathcal{F}$, an event being a set of outcomes in the sample space.\n",
    "* A probability function, which assigns each event in the event space a probability, which is a number between $0$ and $1$.\n",
    "\n",
    "## Examples\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Range\n",
    "---\n",
    "\n",
    "## Formula\n",
    "\n",
    "$\\Huge x_{n} - x_{\\Large 1}$\n",
    "\n",
    "The range of a data set is the difference between its minimum and maximum values.\n",
    "\n",
    "## Examples\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Numbers\n",
    "---\n",
    "\n",
    "## Symbol\n",
    "\n",
    "$\n",
    "\\LARGE\n",
    "\\mathbb{R}\n",
    "$\n",
    "\n",
    "A real number is a value of a continuous quantity that can represent a distance along a line (or alternatively, a quantity that can be represented as an infinite decimal expansion).\n",
    "\n",
    "The real numbers ($\\mathbb{R}$) include the rational numbers ($\\mathbb{Q}$), which themselves include the integers ($\\mathbb{Z}$), which in turn include the natural numbers ($\\mathbb{N}$).\n",
    "\n",
    "![Number systems](images\\number_systems.png \"Number systems\")\n",
    "\n",
    "Therefore, the real numbers include all the rational numbers, such as the integer $−5$ and the fraction $4/3$, and all the irrational numbers, such as $\\sqrt{2}$ (the square root of $2$). Included within the irrationals are the transcendental numbers, such as $\\large \\pi$.\n",
    "\n",
    "The adjective real in this context was introduced in the 17<sup>th</sup> century by René Descartes, who distinguished between real and imaginary roots of polynomials.\n",
    "\n",
    "In addition to measuring distance, real numbers can be used to measure quantities such as time, mass, energy, velocity, and many more.\n",
    "\n",
    "$\\mathbb{R}^{n}$ is a coordinate space over the real numbers. This means that it is the set of the $n$-tuples of real numbers (sequences of $n$ real numbers). With component-wise addition and scalar multiplication, it is a real vector space.\n",
    "\n",
    "Typically, the Cartesian coordinates of the elements of a Euclidean space form a real coordinate space. This explains the name of coordinate space and the fact that geometric terms are often used when working with coordinate spaces.\n",
    "\n",
    "For example, $\\mathbb{R}^{2}$ is a plane (a two-dimensional flat surface that extends infinitely far):\n",
    "\n",
    " * $x$-axis $= \\{(x, y) \\in \\mathbb{R}^{2} : y = 0\\}$\n",
    " * $y$-axis $= \\{(x, y) \\in \\mathbb{R}^{2} : x = 0\\}$\n",
    "\n",
    "Coordinate spaces are widely used in geometry and physics, as their elements allow locating points in Euclidean spaces, and computing with them.\n",
    "\n",
    "## Examples\n",
    "\n",
    "The number $2$ is an element of the Real Numbers set:\n",
    "\n",
    "$\n",
    "\\large\n",
    "2 \\in \\mathbb{R}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity and Specificity\n",
    "---\n",
    "\n",
    "Sensitivity and specificity are statistical measures of the performance of a binary classification test that are widely used in medicine. The terms were introduced by American biostatistician Jacob Yerushalmy in 1947:\n",
    "\n",
    "* Sensitivity measures the proportion of positives that are correctly identified (e.g., the percentage of sick people who are correctly identified as having some illness).\n",
    "\n",
    "* Specificity measures the proportion of negatives that are correctly identified (e.g., the percentage of healthy people who are correctly identified as not having some illness).\n",
    "\n",
    "The terms _positive_ and _negative_ do not refer to benefit, but to the presence or absence of a condition. For example, if the condition is a disease, \"positive\" means \"diseased\" and \"negative\" means \"healthy\".\n",
    "\n",
    "In many tests, including diagnostic medical tests, sensitivity is the extent to which true positives are not overlooked, thus false negatives are few, and specificity is the extent to which true negatives are classified as such, thus false positives are few. A sensitive test rarely overlooks a true positive (for example, showing nothing wrong despite a problem existing); a specific test rarely registers a positive classification for anything that is not the target of testing (for example, finding one bacterial species and mistaking it for another closely related one that is the true target).\n",
    "\n",
    "A perfect predictor would be $100\\%$ sensitive, meaning all sick individuals are correctly identified as sick, and $100\\%$ specific, meaning no healthy individuals are incorrectly identified as sick.\n",
    "\n",
    "There is usually a trade-off between measures. For instance, in airport security, since testing of passengers is for potential threats to safety, scanners may be set to trigger alarms on low-risk items like belt buckles and keys (low specificity) in order to increase the probability of identifying dangerous objects and minimize the risk of missing objects that do pose a threat (high sensitivity).\n",
    "\n",
    "## Examples\n",
    "\n",
    "![Graphical illustration of sensitivity vs specificity](images\\sensitivity_and_specificity.png \"A graphical illustration of sensitivity and specificity comparisons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Theory\n",
    "---\n",
    "\n",
    "$A$ set is a collection of objects. Each object in the set is an element of the set. The order of the elements in a set does not matter.\n",
    "\n",
    "The cardinality of $A$, sometimes mentioned as the absolute value of $A$, is noted as $|A|$, and refers to the number of elements in $A$.\n",
    "\n",
    "The complement of set $A$, is expressed as $\\overline{A}$, or $A^\\mathsf{c}$, and it refers to all the elements that are not elements of $A$.\n",
    "\n",
    "## Examples\n",
    "\n",
    "Set $A$ contains all integer numbers from $1$ to $10$:\n",
    "\n",
    "$A = \\{1, 2, 3, 4, 5, 6, 7, 8, 9, 10\\}$\n",
    "\n",
    "Number $2$ is an element of set $A$:\n",
    "\n",
    "$2 \\in A$\n",
    "\n",
    "Number $11.4$ is not an element of set $A$:\n",
    "\n",
    "$11.4 \\notin A$\n",
    "\n",
    "Set $B$ contains two sport modalities:\n",
    "\n",
    "$B = \\{$\"baseball\"$, \\, $\"football\"$\\}$\n",
    "\n",
    "Set $C$ refers to the interval in the real numbers line that contains all the numbers between $0$ and $1$ (\"$|$\" or \"$:$\" means \"such that\"):\n",
    "\n",
    "$C = \\{ \\, x \\, | \\, 0 \\leq x \\leq 1 \\, \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific Notation\n",
    "---\n",
    "\n",
    "## Formula\n",
    "\n",
    "$\n",
    "\\LARGE\n",
    "m \\cdot 10^{n}\n",
    "$\n",
    "\n",
    "In scientific notation, all numbers are written in the form \"$m$ times ten raised to the power of $n$\", where the exponent $n$ is an integer, and the coefficient $m$ is any real number.\n",
    "\n",
    "The scientific notation is a way of expressing real numbers that are too large or too small to be conveniently written in decimal form. It may also be referred to as \"scientific form\" or \"standard index form\", or \"standard form\" in the UK.\n",
    "\n",
    "It is a base ten notation, commonly used by scientists, mathematicians, and engineers, in part because it can simplify certain arithmetic operations.\n",
    "\n",
    "## Examples\n",
    "\n",
    "The rule for the conversion from decimal to scientific notation is to keep all the significant digits, and then to always leave one digit to the left of the decimal:\n",
    "\n",
    "![Decimal to scientific notation conversion examples](images\\scientific_notation.png \"A table containing examples of conversions from decimal to scientific notation\")\n",
    "\n",
    "The Earth's mass value (in Kg):\n",
    "\n",
    "$\\large 5,972,000,000,000,000,000,000,000 = 5.972 \\cdot 10^{24}$\n",
    "\n",
    "An electron's mass value (in Kg):\n",
    "\n",
    "$\\large 0.00000000000000000000000000009109 = 9.109 \\cdot 10^{-31}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5.972E+24\n9.109E-31\n5.972E+24\n9.109E-31\n5.972E+24\n9.109E-31\n"
     ]
    }
   ],
   "source": [
    "earth_mass = 5972000000000000000000000\n",
    "electron_mass = 0.0000000000000000000000000000009109\n",
    "\n",
    "# Using format() function:\n",
    "print(format(earth_mass, \".3E\"))\n",
    "print(format(electron_mass, \".3E\"))\n",
    "\n",
    "# Alternative format() syntax:\n",
    "print(\"{:.3E}\".format(earth_mass))\n",
    "print(\"{:.3E}\".format(electron_mass))\n",
    "\n",
    "# Using f-strings:\n",
    "print(f\"{earth_mass:.3E}\")\n",
    "print(f\"{electron_mass:.3E}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid\n",
    "---\n",
    "\n",
    "A sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve.\n",
    "\n",
    "A wide variety of sigmoid functions including the logistic and hyperbolic tangent functions have been used as the activation function of artificial neurons. Sigmoid curves are also common in statistics as cumulative distribution functions (which go from $0$ to $1$), such as the integrals of the logistic density, the normal density, and Student's $t$ probability density functions. The logistic sigmoid function is invertible, and its inverse is the logit function.\n",
    "\n",
    "In general, a sigmoid function is monotonic, and has a first derivative which is bell shaped. Conversely, the integral of any continuous, non-negative, bell-shaped function (with one local maximum and no local minimum, unless degenerate) will be sigmoidal. Thus the cumulative distribution functions for many common probability distributions are sigmoidal. One such example is the error function, which is related to the cumulative distribution function of a normal distribution.\n",
    "\n",
    "A sigmoid function is constrained by a pair of horizontal asymptotes as $x \\rightarrow \\pm \\infty$.\n",
    "\n",
    "A sigmoid function is convex for values less than $0$, and it is concave for values greater than $0$.\n",
    "\n",
    "![Sigmoid Functions](images\\sigmoid_functions.png \"Sigmoid Functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slope\n",
    "---\n",
    "\n",
    "## Formula\n",
    "\n",
    "$\n",
    "\\Large\n",
    "m = \\dfrac{\\Delta y}{\\Delta x} = \\dfrac{y_{2} - y_{1}}{x_{2} - x_{1}}\n",
    "$\n",
    "\n",
    "In mathematics, the slope or gradient of a line is a number that describes both the direction and the steepness of the line. Slope is often denoted by the letter $m$.\n",
    "\n",
    "The slope is calculated by finding the ratio of the \"vertical change\" to the \"horizontal change\" between (any) two distinct points on a line. Sometimes the ratio is expressed as a quotient (\"rise over run\"), giving the same number for every two distinct points on the same line. A line that is decreasing has a negative \"rise\". The line may be practical - as set by a road surveyor, or in a diagram that models a road or a roof either as a description or as a plan.\n",
    "\n",
    "The steepness, incline, or grade of a line is measured by the absolute value of the slope. A slope with a greater absolute value indicates a steeper line.\n",
    "\n",
    "The direction of a line is either increasing, decreasing, horizontal or vertical:\n",
    "\n",
    "* A line is increasing if it goes up from left to right. In this case, the slope is positive: $m > 0$.\n",
    "* A line is decreasing if it goes down from left to right. In this case, the slope is negative: $m < 0$.\n",
    "* If a line is horizontal, the slope is zero. This is a constant function.\n",
    "* If a line is vertical, the slope is undefined.\n",
    "\n",
    "![Slope of a linear function](images\\slope_of_a_linear_function.png \"Slope of a linear function\")\n",
    "\n",
    "## Examples\n",
    "\n",
    "Calculate the slope between point $A = (2, -1)$ and point $B = (4, 3)$:\n",
    "\n",
    "$\n",
    "\\large\n",
    "m = \\Large\\frac{B_{y} \\, - \\, A_{y}}{B_{x} \\, - \\, A_{x}} = \\frac{3 \\, - \\, (-1)}{4 - 2} = \\frac{4}{2} = \\large 2\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slope Intercept Form\n",
    "---\n",
    "\n",
    "## Formula\n",
    "\n",
    "$\\Large y = mx + b$\n",
    "\n",
    "The slope intercept form is used to find the equation of a straight line. The formula identifies the line slope, represented as $m$, and the $y$-intercept, represented as $b$, which has this name because it is where the line intercepts the $y$-axis. \n",
    "\n",
    "The slope measures how steep the line is. It is the inclination, or gradient, of a line. If it is positive, the values of $y$ increase with increasing $x$, and if it is negative, $y$ decreases with an increasing $x$.\n",
    "\n",
    "The $y$-intercept is the place where the line crosses the $y$ axis, so we can also think of the $y$-intercept as the value of $y$ when $x$ equals zero.\n",
    "\n",
    "In summary, we have:\n",
    "\n",
    " * $y$: dependent variable\n",
    " * $m$: slope of the line\n",
    " * $x$: independent variable\n",
    " * $b$: $y$-intercept\n",
    "\n",
    "## Examples\n",
    "\n",
    "### Least-Squares Regression\n",
    "\n",
    "The most common method for fitting a regression line is the method of least-squares. This method calculates the best-fitting line for the observed data by minimizing the sum of the squares of the vertical deviations from each data point to the line (if a point lies on the fitted line exactly, then its vertical deviation is $0$).\n",
    "\n",
    "Because the deviations are first squared, then summed, there are no cancellations between positive and negative values.\n",
    "\n",
    "To calculate the LSR, assuming we have $x$ and $y$ number sets, each one with $n$ data points:\n",
    "\n",
    " * __Step 1__: Calculate the slope ($m$):\n",
    "\n",
    " $\\large m = \\dfrac{n \\sum xy - (\\sum x)(\\sum y)}{n \\sum x^{2} - (\\sum x)^{2}}$\n",
    "\n",
    " * __Step 2__: Calculate the $y$-intercept ($b$):\n",
    "\n",
    " $\\large b = \\dfrac{(\\sum y - m (\\sum{x}))}{n}$\n",
    "\n",
    " * __Step 3__: Calculate the predicted value ($y$):\n",
    "\n",
    " $\\large y = mx + b$\n",
    "\n",
    "Some of the things to consider when implementing the least-squares regression method:\n",
    "\n",
    " * The data must be free of outliers because they might lead to a biased and wrongful line of best fit.\n",
    " * The line of best fit can be drawn iteratively until we get a line with the minimum possible squares of errors.\n",
    " * The method works well even with non-linear data.\n",
    " * Technically, the difference between the actual value of $y$ and the predicted value of $y$ is called a \"residual\" (because it denotes the error).\n",
    "\n",
    "### Root Mean Squared Error\n",
    "\n",
    "The Root Mean Squared Error (RMSE) allows evaluating the quality of a prediction model. It is calculated by getting the square root of the sum of all errors divided by the total number of values. $\\hat{y}_{i}$ is the $_{i}$th predicted value:\n",
    "\n",
    "$\\large RMSE = \\sqrt{\\sum\\limits_{i = 1}^{n} \\dfrac{1}{n}(\\hat{y}_{i} - y_{i})^{2}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "slope: 1.4021526418786692\ny_intercept: 65.1682974559687\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 391.816237 262.19625\" width=\"391.816237pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-01-12T09:52:49.215749</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 262.19625 \r\nL 391.816237 262.19625 \r\nL 391.816237 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 46.965625 224.64 \r\nL 381.765625 224.64 \r\nL 381.765625 7.2 \r\nL 46.965625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m0527d9ac27\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"85.596394\" xlink:href=\"#m0527d9ac27\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 65 -->\r\n      <g transform=\"translate(79.233894 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"144.127863\" xlink:href=\"#m0527d9ac27\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 70 -->\r\n      <g transform=\"translate(137.765363 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"202.659331\" xlink:href=\"#m0527d9ac27\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 75 -->\r\n      <g transform=\"translate(196.296831 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"261.1908\" xlink:href=\"#m0527d9ac27\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 80 -->\r\n      <g transform=\"translate(254.8283 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"319.722268\" xlink:href=\"#m0527d9ac27\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 85 -->\r\n      <g transform=\"translate(313.359768 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"378.253737\" xlink:href=\"#m0527d9ac27\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 90 -->\r\n      <g transform=\"translate(371.891237 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.984375 1.515625 \r\nL 10.984375 10.5 \r\nQ 14.703125 8.734375 18.5 7.8125 \r\nQ 22.3125 6.890625 25.984375 6.890625 \r\nQ 35.75 6.890625 40.890625 13.453125 \r\nQ 46.046875 20.015625 46.78125 33.40625 \r\nQ 43.953125 29.203125 39.59375 26.953125 \r\nQ 35.25 24.703125 29.984375 24.703125 \r\nQ 19.046875 24.703125 12.671875 31.3125 \r\nQ 6.296875 37.9375 6.296875 49.421875 \r\nQ 6.296875 60.640625 12.9375 67.421875 \r\nQ 19.578125 74.21875 30.609375 74.21875 \r\nQ 43.265625 74.21875 49.921875 64.515625 \r\nQ 56.59375 54.828125 56.59375 36.375 \r\nQ 56.59375 19.140625 48.40625 8.859375 \r\nQ 40.234375 -1.421875 26.421875 -1.421875 \r\nQ 22.703125 -1.421875 18.890625 -0.6875 \r\nQ 15.09375 0.046875 10.984375 1.515625 \r\nz\r\nM 30.609375 32.421875 \r\nQ 37.25 32.421875 41.125 36.953125 \r\nQ 45.015625 41.5 45.015625 49.421875 \r\nQ 45.015625 57.28125 41.125 61.84375 \r\nQ 37.25 66.40625 30.609375 66.40625 \r\nQ 23.96875 66.40625 20.09375 61.84375 \r\nQ 16.21875 57.28125 16.21875 49.421875 \r\nQ 16.21875 41.5 20.09375 36.953125 \r\nQ 23.96875 32.421875 30.609375 32.421875 \r\nz\r\n\" id=\"DejaVuSans-57\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-57\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_7\">\r\n     <!-- Weight -->\r\n     <g transform=\"translate(196.946875 252.916562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 3.328125 72.90625 \r\nL 13.28125 72.90625 \r\nL 28.609375 11.28125 \r\nL 43.890625 72.90625 \r\nL 54.984375 72.90625 \r\nL 70.3125 11.28125 \r\nL 85.59375 72.90625 \r\nL 95.609375 72.90625 \r\nL 77.296875 0 \r\nL 64.890625 0 \r\nL 49.515625 63.28125 \r\nL 33.984375 0 \r\nL 21.578125 0 \r\nz\r\n\" id=\"DejaVuSans-87\"/>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n       <path d=\"M 45.40625 27.984375 \r\nQ 45.40625 37.75 41.375 43.109375 \r\nQ 37.359375 48.484375 30.078125 48.484375 \r\nQ 22.859375 48.484375 18.828125 43.109375 \r\nQ 14.796875 37.75 14.796875 27.984375 \r\nQ 14.796875 18.265625 18.828125 12.890625 \r\nQ 22.859375 7.515625 30.078125 7.515625 \r\nQ 37.359375 7.515625 41.375 12.890625 \r\nQ 45.40625 18.265625 45.40625 27.984375 \r\nz\r\nM 54.390625 6.78125 \r\nQ 54.390625 -7.171875 48.1875 -13.984375 \r\nQ 42 -20.796875 29.203125 -20.796875 \r\nQ 24.46875 -20.796875 20.265625 -20.09375 \r\nQ 16.0625 -19.390625 12.109375 -17.921875 \r\nL 12.109375 -9.1875 \r\nQ 16.0625 -11.328125 19.921875 -12.34375 \r\nQ 23.78125 -13.375 27.78125 -13.375 \r\nQ 36.625 -13.375 41.015625 -8.765625 \r\nQ 45.40625 -4.15625 45.40625 5.171875 \r\nL 45.40625 9.625 \r\nQ 42.625 4.78125 38.28125 2.390625 \r\nQ 33.9375 0 27.875 0 \r\nQ 17.828125 0 11.671875 7.65625 \r\nQ 5.515625 15.328125 5.515625 27.984375 \r\nQ 5.515625 40.671875 11.671875 48.328125 \r\nQ 17.828125 56 27.875 56 \r\nQ 33.9375 56 38.28125 53.609375 \r\nQ 42.625 51.21875 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nz\r\n\" id=\"DejaVuSans-103\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n       <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-87\"/>\r\n      <use x=\"93.001953\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"154.525391\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"182.308594\" xlink:href=\"#DejaVuSans-103\"/>\r\n      <use x=\"245.785156\" xlink:href=\"#DejaVuSans-104\"/>\r\n      <use x=\"309.164062\" xlink:href=\"#DejaVuSans-116\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"ma205d129b5\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#ma205d129b5\" y=\"207.025228\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 155 -->\r\n      <g transform=\"translate(20.878125 210.824446)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#ma205d129b5\" y=\"181.357445\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 160 -->\r\n      <g transform=\"translate(20.878125 185.156664)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#ma205d129b5\" y=\"155.689662\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 165 -->\r\n      <g transform=\"translate(20.878125 159.488881)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#ma205d129b5\" y=\"130.02188\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 170 -->\r\n      <g transform=\"translate(20.878125 133.821099)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#ma205d129b5\" y=\"104.354097\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 175 -->\r\n      <g transform=\"translate(20.878125 108.153316)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#ma205d129b5\" y=\"78.686315\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 180 -->\r\n      <g transform=\"translate(20.878125 82.485533)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#ma205d129b5\" y=\"53.018532\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 185 -->\r\n      <g transform=\"translate(20.878125 56.817751)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#ma205d129b5\" y=\"27.350749\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 190 -->\r\n      <g transform=\"translate(20.878125 31.149968)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-57\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_16\">\r\n     <!-- Height -->\r\n     <g transform=\"translate(14.798438 132.448906)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 9.8125 72.90625 \r\nL 19.671875 72.90625 \r\nL 19.671875 43.015625 \r\nL 55.515625 43.015625 \r\nL 55.515625 72.90625 \r\nL 65.375 72.90625 \r\nL 65.375 0 \r\nL 55.515625 0 \r\nL 55.515625 34.71875 \r\nL 19.671875 34.71875 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-72\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"75.195312\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"136.71875\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"164.501953\" xlink:href=\"#DejaVuSans-103\"/>\r\n      <use x=\"227.978516\" xlink:href=\"#DejaVuSans-104\"/>\r\n      <use x=\"291.357422\" xlink:href=\"#DejaVuSans-116\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <defs>\r\n     <path d=\"M 0 5 \r\nC 1.326016 5 2.597899 4.473168 3.535534 3.535534 \r\nC 4.473168 2.597899 5 1.326016 5 0 \r\nC 5 -1.326016 4.473168 -2.597899 3.535534 -3.535534 \r\nC 2.597899 -4.473168 1.326016 -5 0 -5 \r\nC -1.326016 -5 -2.597899 -4.473168 -3.535534 -3.535534 \r\nC -4.473168 -2.597899 -5 -1.326016 -5 0 \r\nC -5 1.326016 -4.473168 2.597899 -3.535534 3.535534 \r\nC -2.597899 4.473168 -1.326016 5 0 5 \r\nz\r\n\" id=\"m6b947f8a8c\" style=\"stroke:#1f77b4;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#pc52acb5f5e)\">\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"62.183807\" xlink:href=\"#m6b947f8a8c\" y=\"207.025228\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"366.547443\" xlink:href=\"#m6b947f8a8c\" y=\"17.083636\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"237.778212\" xlink:href=\"#m6b947f8a8c\" y=\"124.888323\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#pc52acb5f5e)\" d=\"M 62.183807 214.756364 \r\nL 366.547443 27.627961 \r\nL 237.778212 106.79767 \r\n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 46.965625 224.64 \r\nL 46.965625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 381.765625 224.64 \r\nL 381.765625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 46.965625 224.64 \r\nL 381.765625 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 46.965625 7.2 \r\nL 381.765625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 53.965625 44.55625 \r\nL 150.8 44.55625 \r\nQ 152.8 44.55625 152.8 42.55625 \r\nL 152.8 14.2 \r\nQ 152.8 12.2 150.8 12.2 \r\nL 53.965625 12.2 \r\nQ 51.965625 12.2 51.965625 14.2 \r\nL 51.965625 42.55625 \r\nQ 51.965625 44.55625 53.965625 44.55625 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_17\"/>\r\n    <g id=\"line2d_18\">\r\n     <g>\r\n      <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"65.965625\" xlink:href=\"#m6b947f8a8c\" y=\"20.298437\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_17\">\r\n     <!-- Original data -->\r\n     <g transform=\"translate(83.965625 23.798437)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 39.40625 66.21875 \r\nQ 28.65625 66.21875 22.328125 58.203125 \r\nQ 16.015625 50.203125 16.015625 36.375 \r\nQ 16.015625 22.609375 22.328125 14.59375 \r\nQ 28.65625 6.59375 39.40625 6.59375 \r\nQ 50.140625 6.59375 56.421875 14.59375 \r\nQ 62.703125 22.609375 62.703125 36.375 \r\nQ 62.703125 50.203125 56.421875 58.203125 \r\nQ 50.140625 66.21875 39.40625 66.21875 \r\nz\r\nM 39.40625 74.21875 \r\nQ 54.734375 74.21875 63.90625 63.9375 \r\nQ 73.09375 53.65625 73.09375 36.375 \r\nQ 73.09375 19.140625 63.90625 8.859375 \r\nQ 54.734375 -1.421875 39.40625 -1.421875 \r\nQ 24.03125 -1.421875 14.8125 8.828125 \r\nQ 5.609375 19.09375 5.609375 36.375 \r\nQ 5.609375 53.65625 14.8125 63.9375 \r\nQ 24.03125 74.21875 39.40625 74.21875 \r\nz\r\n\" id=\"DejaVuSans-79\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n       <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n       <path id=\"DejaVuSans-32\"/>\r\n       <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-79\"/>\r\n      <use x=\"78.710938\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"119.824219\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"147.607422\" xlink:href=\"#DejaVuSans-103\"/>\r\n      <use x=\"211.083984\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"238.867188\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"302.246094\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"363.525391\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"391.308594\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"423.095703\" xlink:href=\"#DejaVuSans-100\"/>\r\n      <use x=\"486.572266\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"547.851562\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"587.060547\" xlink:href=\"#DejaVuSans-97\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_19\">\r\n     <path d=\"M 55.965625 34.976562 \r\nL 75.965625 34.976562 \r\n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_20\"/>\r\n    <g id=\"text_18\">\r\n     <!-- Fitted line -->\r\n     <g transform=\"translate(83.965625 38.476562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 9.8125 72.90625 \r\nL 51.703125 72.90625 \r\nL 51.703125 64.59375 \r\nL 19.671875 64.59375 \r\nL 19.671875 43.109375 \r\nL 48.578125 43.109375 \r\nL 48.578125 34.8125 \r\nL 19.671875 34.8125 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-70\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-70\"/>\r\n      <use x=\"50.269531\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"78.052734\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"117.261719\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"156.470703\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"217.994141\" xlink:href=\"#DejaVuSans-100\"/>\r\n      <use x=\"281.470703\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"313.257812\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"341.041016\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"368.824219\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"432.203125\" xlink:href=\"#DejaVuSans-101\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pc52acb5f5e\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"46.965625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The prediction for x = 60 is y = 149.29\n"
     ]
    }
   ],
   "source": [
    "# A regression model to predict people's height based on their weight\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Weight (Kg)\n",
    "X = np.array([63, 89, 78])\n",
    "# Height (cm)\n",
    "Y = np.array([155, 192, 171])\n",
    "\n",
    "N = len(X)\n",
    "\n",
    "# Step 1:\n",
    "# Calculate the slope using Least-Squares Regression \n",
    "slope = ((N * np.dot(X, Y)) - np.sum(X) * np.sum(Y)) / \\\n",
    "    ((N * np.sum(X ** 2)) - (np.sum(X) ** 2))\n",
    "print(f\"slope: {slope}\")\n",
    "\n",
    "# Step 2:\n",
    "# Calculate the y-intercept\n",
    "y_intercept = (np.sum(Y) - (slope * np.sum(X))) / N\n",
    "print(f\"y_intercept: {y_intercept}\")\n",
    "\n",
    "slope = round(slope, 3)\n",
    "y_intercept = round(y_intercept, 3)\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(X, Y, \"o\", label=\"Original data\", markersize=10)\n",
    "plt.plot(X, slope * X + y_intercept, \"r\", label=\"Fitted line\")\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Height\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# A sample input weight\n",
    "x = 60\n",
    "\n",
    "# Step 3:\n",
    "# Predict a person's height based on his weight\n",
    "y = slope * x + y_intercept\n",
    "y = round(y, 2)\n",
    "\n",
    "# The prediction for a person with 60Kg weight is 149.29cm height \n",
    "print(f\"The prediction for x = {x} is y = {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Deviation\n",
    "---\n",
    "\n",
    "## Formula\n",
    "\n",
    "$\\Huge \\sigma = \\Large \\sqrt{ \\dfrac{1}{N} \\sum\\limits_{i=1}^N (x_i -\\mu)^2}$\n",
    "\n",
    "In statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range.\n",
    "\n",
    "Standard deviation may be abbreviated as __SD__. The $\\Large\\sigma$ or $\\Large\\sigma_{x}$ symbols are used to express a population's standard deviation. The $\\Large s$ symbol is used to express the standard deviation of a sample of a population, which then can be referred as the _standard deviation of the sample_ or _sample standard deviation_. \n",
    "\n",
    "The standard deviation of a random variable, statistical population, data set, or probability distribution is the square root of its variance. It is algebraically simpler, though in practice less robust, than the average absolute deviation. A useful property of the standard deviation is that unlike the variance, it is expressed in the same unit as the data.\n",
    "\n",
    "In addition to expressing the variability of a population, the standard deviation is commonly used to measure confidence in statistical conclusions. For example, the margin of error in polling data is determined by calculating the expected standard deviation in the results if the same poll were to be conducted multiple times.\n",
    "\n",
    "This derivation of a standard deviation is often called the \"standard error of the estimate\", or \"standard error of the mean\" when referring to a mean. It is computed as the standard deviation of all the means that would be computed from that population, if an infinite number of samples were drawn and a mean for each sample were computed.\n",
    "\n",
    "To calculate standard deviation, add up all the data points and divide by the number of data points, calculate the variance for each data point and then find the square root of the variance.\n",
    "\n",
    "## Examples\n",
    "\n",
    "To calculate a population's standard deviation, divide the sum result by the total of elements in the dataset:\n",
    "\n",
    "$\\Large \\sigma = \\sqrt{\\frac{\\sum(X_{i}-\\mu_{x})^{2}}{n}}$\n",
    "\n",
    "To calculate the standard deviation of a sample of a population, we do the same as previous, except that we need to divide by the total of elements in the dataset, minus one:\n",
    "\n",
    "$\\Large s = \\sqrt{\\frac{\\sum(X_{i}-\\bar{X})^{2}}{n-1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum Of Squares\n",
    "---\n",
    "\n",
    "## Sum Of Squares Total (SST or TSS) \n",
    "\n",
    "$\\Large \\sum\\limits_{i=1}^{n} (y_{i} - \\overline{y})^{2}$\n",
    "\n",
    "The SST, also designated Total Sum of Squares (TSS), is the squared differences total between the observed dependent variables ($y_{i}$) and its mean ($\\overline{y}$). It is the dispersion of the observed variables around the mean. Like the variance in descriptive statistics, it is a measure of the total variability of the dataset.\n",
    "\n",
    "![Sum of Squares Total](images\\sst.png \"Sum of Squares Total\")\n",
    "\n",
    "## Sum Of Squares Regression (SSR or ESS) \n",
    "\n",
    "$\\Large \\sum\\limits_{i=1}^{n} (\\hat{y}_{i} - \\overline{y})^{2}$\n",
    "\n",
    "The SSR, also designated as Explained Sum of Squares (ESS), is the sum of squares due to regression. It is the sum of the differences between the predicted values ($\\hat{y}_{i}$) and the mean of the dependent variables ($\\overline{y}$). It is a measure that describes how well the regression line fits the data.\n",
    "\n",
    "![Sum of Squares Regression](images\\ssr.png \"Sum of Squares Regression\")\n",
    "\n",
    "## Sum Of Squares Error (SSE or RSS) \n",
    "\n",
    "$\\Large \\sum\\limits_{i=1}^{n} e_{i}^{2}$\n",
    "\n",
    "The SSE, also designated as the Residual Sum of Squares (RSS), describes the error as the difference between the observed values ($y_{i}$) and the predicted values ($\\hat{y}_{i}$). Residual, as in: remaining or unexplained. The smaller the total error value, the better the estimation quality of the model.\n",
    "\n",
    "![Sum of Squares Error](images\\sse.png \"Sum of Squares Error\")\n",
    "\n",
    "## Examples\n",
    "\n",
    "The total variability of the data set is equal to the variability explained by the regression line plus the unexplained variability, known as error: SST = SSR + SSE.\n",
    "\n",
    "Given a constant total variability, a lower error will cause a better regression. Conversely, a higher error will cause a less good regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Summation\n",
    "---\n",
    "\n",
    "## Symbol\n",
    "\n",
    "$\\Large \\sum\\limits_{i=1}^n i$\n",
    "\n",
    "In mathematics, summation is the addition of a sequence of any kind of numbers, called addends or summands. The result is their sum or total. Beside numbers, other types of values can be summed as well: functions, vectors, matrices, polynomials and, in general, elements of any type of mathematical objects on which an operation denoted \"+\" is defined.\n",
    "\n",
    "The mathematical notation uses an enlarged form of the upright capital Greek letter Sigma to compactly represent summation of many similar terms, naming it the \"summation symbol\". \n",
    "\n",
    "The summation operation is defined as:\n",
    "\n",
    "$\\large \\sum\\limits_{i = m}^n \\large a_i = a_m + a_{m+1} + a_{m+2} +\\cdots+ a_{n-1} + a_n$\n",
    "\n",
    "Where $i$ is the index of summation, $a_i$ is an indexed variable representing each term of the sum, $m$ is the lower bound of summation, and $n$ is the upper bound of summation. The \"$i=m$\" under the summation symbol means that the index $i$ starts out equal to $m$. The index $i$ is incremented by one for each successive term, stopping when $i=n$.\n",
    "\n",
    "## Examples\n",
    "\n",
    "A sum of squares:\n",
    "\n",
    "$\\Large \\sum\\limits_{i=3}^6 \\normalsize i^2 = 3^2 + 4^2 + 5^2 + 6^2 = 86$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "A = [2.12, 0.9, 1.98]\n",
    "\n",
    "# Use math.fsum() instead of sum() to achieve\n",
    "# higher precision with floating point numbers\n",
    "math.fsum(A) == 5.0"
   ]
  },
  {
   "source": [
    "# Support Vector Machine\n",
    "---\n",
    "## Formulas\n",
    "\n",
    "$\\LARGE \\min\\limits_{\\theta} C \\sum\\limits_{i=1}^{m} y^{(i)} \\, cost_{1}(\\theta^{T} f^{(i)}) + (1 - y^{(i)}) \\, cost_{0}(\\theta^{T} f^{(i)}) + \\frac{1}{2} \\sum\\limits_{j=1}^{m} \\theta_{j}^{2}$\n",
    "\n",
    "or\n",
    "\n",
    "$\\LARGE \\min\\limits_{\\theta} C \\sum\\limits_{i=1}^{m} y^{(i)} \\, cost_{1}(\\theta^{T} f^{(i)}) + (1 - y^{(i)}) \\, cost_{0}(\\theta^{T} f^{(i)}) + \\theta^{T} \\theta$\n",
    "\n",
    "In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis.\n",
    "\n",
    "Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier.\n",
    "\n",
    "An SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\n",
    "\n",
    "A kernel is a function that expresses a measure of similarity between vectors. In addition to performing a linear classification, SVMs can efficiently perform a non-linear classification using what is called the \"kernel trick\", implicitly mapping their inputs into high-dimensional feature spaces.\n",
    "\n",
    "## Examples\n",
    "\n",
    "### Linear Kernel\n",
    "\n",
    "$\\Large (\\bar{u} \\cdot \\bar{v} + 1)^{n}$\n",
    "\n",
    "### Polynomial Kernel\n",
    "\n",
    "$\\Large (x_{i} \\cdot x_{j} + 1)^{d}$\n",
    "\n",
    "The polynomial kernel is popular in image processing, where $d$ in its equation, is the degree of the polynomial.\n",
    "\n",
    "### Gaussian Kernel\n",
    "\n",
    "$\\large \\exp \\left( - \\dfrac{\\parallel x - y \\parallel ^{2}}{2 \\sigma ^{2}} \\right)$\n",
    "\n",
    "The Gaussian kernel is a general-purpose kernel, most used when there is no prior knowledge about the data.\n",
    "\n",
    "### Radial Basis Function (RBF) Kernel\n",
    "\n",
    "$\\Large e^{ - \\gamma \\, (a \\, - \\, b)^{2}}$\n",
    "\n",
    "One way to deal with overlapping data is to use a SVM with a RBF kernel. The word \"kernel\" is used in mathematics to denote a weighting function for a weighted sum or integral. The RBF kernel measures the distance between the unlabeled input $x'$ and each of the training inputs $x_{i}$, so that the nearest data points have more influence over the classification decision boundary. In this aspect, the RBF kernel behaves like a Weighted Nearest Neighbor model.\n",
    "\n",
    "The weight of that influence is conditioned by $\\gamma$'s value, which in RBF sets the width of the bell-shaped curve. This width scales the squared distance between vectors, thus scaling its influence over the decision boundary determination.\n",
    "\n",
    "For example, assuming we have two data point values of $2.5$ and $4$, and a $\\gamma = 1$, the result is:\n",
    "\n",
    "$\\Large e^{ - 1 \\, (2.5 \\, - \\, 4)^{2}} \\large = 0.1054$\n",
    "\n",
    "Because the closest observations have a lot more influence on how new observations are classified, and the observations that are further away have relatively little influence on the classification, as we increase our input weight of $\\gamma$, the result value we get is much lower:\n",
    "\n",
    "$\\Large e^{ - 4 \\, (2.5 \\, - \\, 4)^{2}} \\large = 0.0001$\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables\n",
    "\n",
    "In Statistics, depending on the context, an independent variable is sometimes called a \"predictor variable\", regressor, covariate, \"manipulated variable\", \"explanatory variable\", \"exposure variable\" (v. reliability theory), \"risk factor\" (v. medical statistics), \"feature\" (in machine learning and pattern recognition) or \"input variable\".\n",
    "\n",
    "In econometrics, the term \"control variable\" is usually used instead of \"covariate\". From the Economics community, the independent variables are also called \"exogenous\".\n",
    "\n",
    "Depending on the context, a dependent variable is sometimes called a \"response variable\", \"regressand\", \"criterion\", \"predicted variable\", \"measured variable\", \"explained variable\", \"experimental variable\", \"responding variable\", \"outcome variable\", \"output variable\", \"target\" or \"label\".\n",
    "\n",
    "In economics, endogenous variables are usually referencing the target.\n",
    "\n",
    "\"Explanatory variable\" is preferred by some authors over \"independent variable\" when the quantities treated as independent variables may not be statistically independent or independently manipulable by the researcher. If the independent variable is referred to as an \"explanatory variable\" then the term \"response variable\" is preferred by some authors for the dependent variable.\n",
    "\n",
    "\"Explained variable\" is preferred by some authors over \"dependent variable\" when the quantities treated as \"dependent variables\" may not be statistically dependent. If the dependent variable is referred to as an \"explained variable\" then the term \"predictor variable\" is preferred by some authors for the independent variable.\n",
    "\n",
    "Variables may also be referred to by their form: continuous or categorical, which in turn may be binary/dichotomous, nominal categorical, and ordinal categorical, among others.\n",
    "\n",
    "A variable may be thought to alter the dependent or independent variables, but may not actually be the focus of the experiment. So that the variable will be kept constant or monitored to try to minimize its effect on the experiment. Such variables may be designated as either a \"controlled variable\", \"control variable\", or \"fixed variable\".\n",
    "\n",
    "Extraneous variables, if included in a regression analysis as independent variables, may aid a researcher with accurate response parameter estimation, prediction, and goodness of fit, but are not of substantive interest to the hypothesis under examination. For example, in a study examining the effect of post-secondary education on lifetime earnings, some extraneous variables might be gender, ethnicity, social class, genetics, intelligence, age, and so forth.\n",
    "\n",
    "A variable is extraneous only when it can be assumed (or shown) to influence the dependent variable. If included in a regression, it can improve the fit of the model. If it is excluded from the regression and if it has a non-zero covariance with one or more of the independent variables of interest, its omission will bias the regression's result for the effect of that independent variable of interest. This effect is called confounding or omitted variable bias; in these situations, design changes and/or controlling for a variable statistical control is necessary.\n",
    "\n",
    "Extraneous variables are often classified into three types:\n",
    "\n",
    " 1. Subject variables, which are the characteristics of the individuals being studied that might affect their actions. These variables include age, gender, health status, mood, background, etc.\n",
    "\n",
    " 2. Blocking variables or experimental variables are characteristics of the persons conducting the experiment which might influence how a person behaves. Gender, the presence of racial discrimination, language, or other factors may qualify as such variables.\n",
    "\n",
    " 3. Situational variables are features of the environment in which the study or research was conducted, which have a bearing on the outcome of the experiment in a negative way. Included are the air temperature, level of activity, lighting, and the time of day.\n",
    "\n",
    "In modelling, variability that is not covered by the independent variable is designated by $e_{I}$ and is known as the \"residual\", \"side effect\", \"error\", \"unexplained share\", \"residual variable\", \"disturbance\", or \"tolerance\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance\n",
    "---\n",
    "\n",
    "## Symbol\n",
    "\n",
    "$\n",
    "\\Huge\n",
    "\\sigma\\Large^2\n",
    "$\n",
    "or\n",
    "$\n",
    "\\Huge\n",
    "s\\Large^2\n",
    "$\n",
    "\n",
    "In probability theory and statistics, variance is the expectation of the squared deviation of a random variable from its mean. Informally, it measures how far a set of numbers is spread out from their average value.\n",
    "\n",
    "The variance, also known as \"variation\" or \"sum of squares\", has a central role in statistics, where some ideas that use it include descriptive statistics, statistical inference, hypothesis testing, goodness of fit, and Monte Carlo sampling.\n",
    "\n",
    "Variance is also an important tool in the sciences, where statistical analysis of data is common. The variance is the square of the standard deviation, the second central moment of a distribution, and the covariance of the random variable with itself.\n",
    "\n",
    "To express a population's variance, use the $\\Large\\sigma\\large^2$ symbol. To express the variance of a sample of a population, use the $\\Large s\\large^2$ symbol.\n",
    "\n",
    "The variance is the average of the squared differences from the mean. To figure out the variance, first calculate the difference between each point and the mean. Then, square and average the results.\n",
    "\n",
    "Because of this squaring, the variance is no longer in the same unit of measurement as the original data. Taking the root of the variance means the standard deviation is restored to the original unit of measure and therefore much easier to interpret.\n",
    "\n",
    "## Examples\n",
    "\n",
    "Calculate the variance in set Z:\n",
    "\n",
    "$\n",
    "Z = \\{6, 8, 10, 7, 8, 13, 9, 10\\}\n",
    "$\n",
    "\n",
    "Calculate the variance of a sample of the population:\n",
    "$\n",
    "\\Large\n",
    "\\sigma\\large^2_{z}\n",
    "\\Large\n",
    "= \\frac{1}{n-1}\\sum\\limits_{i=1}^n(z_{i}-\\mu_{z})^{2}\n",
    "\\normalsize\n",
    "= 4.696428571428571\n",
    "$\n",
    "\n",
    "Calculate the variance of the population:\n",
    "$\n",
    "\\Large\n",
    "\\sigma\\large^2_{z}\n",
    "\\LARGE \n",
    "= \\frac{\\sum\\limits_{i=1}^n(z_{i}-\\mu_{z})^{2}}{n}\n",
    "\\normalsize\n",
    "= 4.109375\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4.696428571428571\n4.109375\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "A = [6, 8, 10, 7, 8, 13, 9, 10]\n",
    "\n",
    "# Variance of a sample of the population\n",
    "sVar_A = statistics.variance(A)\n",
    "\n",
    "# Output is 4.696428571428571\n",
    "print(sVar_A)\n",
    "\n",
    "# Variance of a population\n",
    "pVar_A = statistics.pvariance(A)\n",
    "\n",
    "# Output is 4.109375\n",
    "print(pVar_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Venn Diagram\n",
    "---\n",
    "\n",
    "A Venn diagram, also called primary diagram, set diagram or logic diagram, is a diagram that shows all possible logical relations between a finite collection of different sets. These diagrams depict elements as points in the plane, and sets as regions inside closed curves.\n",
    "\n",
    "A Venn diagram consists of multiple overlapping closed curves, usually circles, each representing a set. The points inside a curve labelled $S$ represent elements of the set $S$, while points outside the boundary represent elements not in the set $S$. This lends itself to intuitive visualizations; for example, the set of all elements that are members of both sets $S$ and $T$, denoted $S \\cap T$ and read \"the intersection of $S$ and $T$\", is represented visually by the area of overlap of the regions $S$ and $T$.\n",
    "\n",
    "In Venn diagrams, the curves are overlapped in every possible way, showing all possible relations between the sets. They are thus a special case of Euler diagrams, which do not necessarily show all relations. A Venn diagram in which the area of each shape is proportional to the number of elements it contains is called an area-proportional (or scaled Venn diagram).\n",
    "\n",
    "Venn diagrams were conceived around 1880 by John Venn. They are used to teach elementary set theory, as well as illustrate simple set relationships in probability, logic, statistics, linguistics, and computer science.\n",
    "\n",
    "## Examples\n",
    "\n",
    "![venn diagram examples](images\\venn_diagrams.png \"Venn diagram examples\")\n",
    "\n",
    "### Complement Laws:\n",
    "\n",
    "* $A \\cup A^\\mathsf{c} = U$\n",
    "* $A \\cap A^\\mathsf{c} =\\varnothing$\n",
    "* $\\varnothing^\\mathsf{c} =U$\n",
    "* $U^\\mathsf{c} = \\varnothing$\n",
    "* If $A\\subseteq B$, then $B^\\mathsf{c}\\subseteq A^\\mathsf{c}$\n",
    "\n",
    "### Commutative Laws:\n",
    "\n",
    "* $A \\cup B = B \\cup A$\n",
    "* $A \\cap B = B \\cap A$\n",
    "\n",
    "### Associative Laws:\n",
    "\n",
    "* $A \\cup (B \\cup C) = (A \\cup B) \\cup C = A \\cup B \\cup C$\n",
    "* $A \\cap (B \\cap C) = (A \\cap B) \\cap C = A \\cap B \\cap C$\n",
    "\n",
    "### Distributive Laws:\n",
    "\n",
    "* $A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C)$\n",
    "* $A \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C)$\n",
    "\n",
    "### De Morgan's Laws:\n",
    "\n",
    "* $\\overline{A \\cup B} = \\overline{A} \\cap \\overline{B}$\n",
    "* $\\overline{A \\cap B} = \\overline{A} \\cup \\overline{B}$\n",
    "* $\\left(A \\cup B \\right)^\\mathsf{c} = A^\\mathsf{c} \\cap B^\\mathsf{c}$\n",
    "* $\\left(A \\cap B \\right)^\\mathsf{c} = A^\\mathsf{c}\\cup B^\\mathsf{c}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Intersection of two sets (using A & B): {2, 4}\nIntersection of two sets (using A.intersection(B)): {2, 4}\n\nUnion of two sets (using A | B:) {0, 1, 2, 3, 4, 5, 6, 8}\nUnion of two sets (using A.union(B)): {0, 1, 2, 3, 4, 5, 6, 8}\n\nSymmetric difference of two sets (using A ^ B): {0, 1, 3, 5, 6, 8}\nSymmetric difference of two sets (using A.symmetric_difference(B)): {0, 1, 3, 5, 6, 8}\n\nRelative complement of A in B (using B - A): {1, 3, 5}\nRelative complement of A in B (using B.difference(A)): {1, 3, 5}\n"
     ]
    }
   ],
   "source": [
    "A = set({0, 2, 4, 6, 8})\n",
    "B = set({1, 2, 3, 4, 5})\n",
    "  \n",
    "print(\"Intersection of two sets (using A & B):\", A & B)\n",
    "print(\"Intersection of two sets (using A.intersection(B)):\", \n",
    "    A.intersection(B))\n",
    "print()\n",
    "print(\"Union of two sets (using A | B:)\", A | B)\n",
    "print(\"Union of two sets (using A.union(B)):\", \n",
    "    A.union(B))\n",
    "print()\n",
    "print(\"Symmetric difference of two sets (using A ^ B):\", A ^ B)\n",
    "print(\"Symmetric difference of two sets (using A.symmetric_difference(B)):\", \n",
    "    A.symmetric_difference(B))\n",
    "print()\n",
    "print(\"Relative complement of A in B (using B - A):\", B - A)\n",
    "print(\"Relative complement of A in B (using B.difference(A)):\", \n",
    "    B.difference(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Y Hat\n",
    "---\n",
    "\n",
    "## Symbol\n",
    "\n",
    "$\n",
    "\\Huge\n",
    "\\hat{y}\n",
    "$\n",
    "\n",
    "Y hat (written $\\hat{y}$) is the predicted value of $y$ (the dependent variable) in a regression equation. It can also be considered to be the average value of the response variable.\n",
    "\n",
    "The regression equation is just the equation which models the data set. The equation is calculated during regression analysis. A simple linear regression equation can be written as:\n",
    "\n",
    "$\\Large\\hat{y} = b_{0} + b_{1}x$\n",
    "\n",
    "Since $b_{0}$ and $b_{1}$ are constants defined by the analysis, finding $\\hat{y}$ for any particular point simply involves plugging in the relevant value of $x$.\n",
    "\n",
    "## Example\n",
    "\n",
    "Supposing we want to predict first grade reading abilities from the number of hours per week a child spends reading in preschool, we would have a set of data points: reading ability scores (assuming to use an unbiased scale) and survey data from homes which would tell us how many hours per day of preschool reading.\n",
    "\n",
    "Having this information, we can use simple linear regression and the least squares method to find the regression equation that best would fit the data.\n",
    "\n",
    "Assuming our line is:\n",
    "\n",
    "$\\hat{y} = 2.45 x -0.16$\n",
    "\n",
    "Let's say $\\hat{y}$ is the predicted average reading level for a child who has read $^1/_2$ an hour a day in preschool. To find this value, we just need to plug in $x = 0.5$:\n",
    "\n",
    "$\\hat{y} = 2.45 (0.5) -0.16 = 1.065$\n",
    "\n",
    "The resulting regression line predicts that a child who reads to half an hour a day in preschool would have a $1.065$ reading level in kindergarten (according to the above mentioned scale)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NOTB",
   "language": "python",
   "name": "notb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}